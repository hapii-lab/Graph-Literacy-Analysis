{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116e50c0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe790860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4bb10",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e969b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "DATA_PATH = \"DGMs v3/3s\"\n",
    "FEATURE_COLUMNS = None\n",
    "\n",
    "# BERT configuration\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 100\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Cross-validation\n",
    "N_SPLITS = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb9488",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeTrackingDataset(Dataset):\n",
    "    def __init__(self, data_path, feature_columns, tokenizer, max_length=512, normalize=True):\n",
    "        self.data_path = data_path\n",
    "        self.feature_columns = feature_columns\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.sequences = []\n",
    "        \n",
    "        self._load_data()\n",
    "        self._preprocess_sequences()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load CSV files and extract sequences\"\"\"\n",
    "        print(\"Loading data files...\")\n",
    "        \n",
    "        label_map = {'literate': 1, 'illiterate': 0}\n",
    "        \n",
    "        if self.feature_columns is None:\n",
    "            for label_name in label_map.keys():\n",
    "                folder_path = os.path.join(self.data_path, label_name)\n",
    "                if os.path.exists(folder_path):\n",
    "                    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "                    if csv_files:\n",
    "                        sample_df = pd.read_csv(csv_files[0])\n",
    "                        self.feature_columns = sample_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                        print(f\"Auto-detected {len(self.feature_columns)} numeric columns\")\n",
    "                        print(f\"Feature columns (first 10): {self.feature_columns[:10]}...\" if len(self.feature_columns) > 10 else f\"Feature columns: {self.feature_columns}\")\n",
    "                        break\n",
    "        \n",
    "        for label_name, label_value in label_map.items():\n",
    "            folder_path = os.path.join(self.data_path, label_name)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"Warning: {folder_path} does not exist\")\n",
    "                continue\n",
    "                \n",
    "            csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "            print(f\"Found {len(csv_files)} files in {label_name} folder\")\n",
    "            \n",
    "            for csv_file in tqdm(csv_files, desc=f\"Loading {label_name} files\"):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    feature_data = df[self.feature_columns].copy()\n",
    "                    feature_data = feature_data.fillna(feature_data.mean())\n",
    "                    \n",
    "                    if len(feature_data) > 0:\n",
    "                        self.samples.append(csv_file)\n",
    "                        self.labels.append(label_value)\n",
    "                        self.sequences.append(feature_data.values)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {csv_file}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} samples total\")\n",
    "        print(f\"Label distribution: {pd.Series(self.labels).value_counts().to_dict()}\")\n",
    "    \n",
    "    def _preprocess_sequences(self):\n",
    "        \"\"\"Preprocess sequences for BERT input\"\"\"\n",
    "        print(\"Preprocessing sequences...\")\n",
    "        \n",
    "        if self.normalize:\n",
    "            all_values = np.concatenate([seq.flatten() for seq in self.sequences])\n",
    "            all_values = all_values[np.isfinite(all_values)]\n",
    "            \n",
    "            self.global_mean = np.mean(all_values)\n",
    "            self.global_std = np.std(all_values)\n",
    "            \n",
    "            print(f\"Global mean: {self.global_mean:.4f}, Global std: {self.global_std:.4f}\")\n",
    "        \n",
    "        self.text_sequences = []\n",
    "        \n",
    "        for seq in tqdm(self.sequences, desc=\"Converting to text\"):\n",
    "            if self.normalize and self.global_std > 0:\n",
    "                seq_normalized = (seq - self.global_mean) / self.global_std\n",
    "                seq_normalized = np.clip(seq_normalized, -5, 5)\n",
    "            else:\n",
    "                seq_normalized = seq\n",
    "            \n",
    "            text_tokens = []\n",
    "            \n",
    "            for time_step in seq_normalized:\n",
    "                chunk_size = 5\n",
    "                feature_chunks = []\n",
    "                \n",
    "                for i in range(0, len(time_step), chunk_size):\n",
    "                    chunk = time_step[i:i+chunk_size]\n",
    "                    chunk_str = \" \".join([f\"{val:.3f}\" for val in chunk])\n",
    "                    feature_chunks.append(f\"[{chunk_str}]\")\n",
    "                \n",
    "                text_tokens.extend(feature_chunks)\n",
    "            \n",
    "            text_sequence = \" \".join(text_tokens)\n",
    "            self.text_sequences.append(text_sequence)\n",
    "        \n",
    "        if self.text_sequences:\n",
    "            print(f\"Sample text sequence (first 200 chars): {self.text_sequences[0][:200]}...\")\n",
    "            print(f\"Total tokens in sample: {len(self.text_sequences[0].split())}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8b545",
   "metadata": {},
   "source": [
    "## Initialize Tokenizer and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer\n",
    "print(\"Initializing BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating dataset...\")\n",
    "dataset = EyeTrackingDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    feature_columns=FEATURE_COLUMNS,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32068727",
   "metadata": {},
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(dataset, batch_size, shuffle=True):\n",
    "    \"\"\"Create a DataLoader for the dataset\"\"\"\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0  # Set to 0 for Windows compatibility\n",
    "    )\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(data_loader), correct_predictions / total_predictions\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc834e0",
   "metadata": {},
   "source": [
    "## Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for cross-validation\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "labels = np.array(dataset.labels)\n",
    "\n",
    "# Store results\n",
    "fold_results = []\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(dataset)), labels)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_SPLITS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create train and validation datasets\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = create_data_loader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "    val_loader = create_data_loader(val_dataset, BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "    total_steps = len(train_loader) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=WARMUP_STEPS,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_accuracy = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_results = evaluate_model(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {val_results['loss']:.4f}, Val Acc: {val_results['accuracy']:.4f}\")\n",
    "        print(f\"Val Precision: {val_results['precision']:.4f}, Val Recall: {val_results['recall']:.4f}, Val F1: {val_results['f1']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_results['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_results['accuracy']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model and get final evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    final_results = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    # Store results\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'accuracy': final_results['accuracy'],\n",
    "        'precision': final_results['precision'],\n",
    "        'recall': final_results['recall'],\n",
    "        'f1': final_results['f1']\n",
    "    })\n",
    "    \n",
    "    all_predictions.extend(final_results['predictions'])\n",
    "    all_true_labels.extend(final_results['labels'])\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} Final Results:\")\n",
    "    print(f\"Accuracy: {final_results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {final_results['precision']:.4f}\")\n",
    "    print(f\"Recall: {final_results['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {final_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ae281",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPer-Fold Results:\")\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "print(\"\\nOverall Statistics:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    mean_val = results_df[metric].mean()\n",
    "    std_val = results_df[metric].std()\n",
    "    print(f\"{metric.capitalize()}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "# Calculate overall confusion matrix\n",
    "cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "overall_accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "\n",
    "print(f\"\\nOverall Accuracy (all folds): {overall_accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133454a",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Metrics by fold\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "ax1.bar(x - 1.5*width, results_df['accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "ax1.bar(x - 0.5*width, results_df['precision'], width, label='Precision', alpha=0.8)\n",
    "ax1.bar(x + 0.5*width, results_df['recall'], width, label='Recall', alpha=0.8)\n",
    "ax1.bar(x + 1.5*width, results_df['f1'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Fold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Performance Metrics by Fold (DGMs v3 Data)')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'Fold {i+1}' for i in range(len(results_df))])\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Confusion matrix\n",
    "ax2 = axes[1]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "            xticklabels=['Illiterate', 'Literate'],\n",
    "            yticklabels=['Illiterate', 'Literate'])\n",
    "ax2.set_title('Overall Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plot of metrics across folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_data = [results_df['accuracy'], results_df['precision'], \n",
    "                results_df['recall'], results_df['f1']]\n",
    "plt.boxplot(metrics_data, labels=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "plt.title('Distribution of Metrics Across Folds (DGMs v3 Data)')\n",
    "plt.ylabel('Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe89e9",
   "metadata": {},
   "source": [
    "## Model Analysis and Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e8f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze some sample predictions\n",
    "print(\"\\nSample Predictions Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get a few samples for analysis\n",
    "sample_indices = np.random.choice(len(dataset), size=min(5, len(dataset)), replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = dataset[idx]\n",
    "    true_label = sample['labels'].item()\n",
    "    \n",
    "    # Get prediction (using the last trained model)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "        confidence = probabilities[0][predicted_label].item()\n",
    "    \n",
    "    label_names = {0: 'Illiterate', 1: 'Literate'}\n",
    "    \n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"  True Label: {label_names[true_label]}\")\n",
    "    print(f\"  Predicted: {label_names[predicted_label]} (confidence: {confidence:.3f})\")\n",
    "    print(f\"  Correct: {'✓' if true_label == predicted_label else '✗'}\")\n",
    "    print(f\"  File: {dataset.samples[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7096f38",
   "metadata": {},
   "source": [
    "## Data Statistics and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea036b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sequence lengths and feature statistics\n",
    "sequence_lengths = [len(seq) for seq in dataset.sequences]\n",
    "text_lengths = [len(text.split()) for text in dataset.text_sequences]\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"Number of features: {len(dataset.feature_columns)}\")\n",
    "print(f\"Feature columns (first 10): {dataset.feature_columns[:10]}...\" if len(dataset.feature_columns) > 10 else f\"Feature columns: {dataset.feature_columns}\")\n",
    "\n",
    "print(f\"\\nSequence Length Statistics:\")\n",
    "print(f\"Mean length: {np.mean(sequence_lengths):.1f}\")\n",
    "print(f\"Median length: {np.median(sequence_lengths):.1f}\")\n",
    "print(f\"Min length: {np.min(sequence_lengths)}\")\n",
    "print(f\"Max length: {np.max(sequence_lengths)}\")\n",
    "print(f\"Std deviation: {np.std(sequence_lengths):.1f}\")\n",
    "\n",
    "print(f\"\\nText Sequence Statistics:\")\n",
    "print(f\"Mean text length (tokens): {np.mean(text_lengths):.1f}\")\n",
    "print(f\"Max text length (tokens): {np.max(text_lengths)}\")\n",
    "print(f\"Sequences exceeding BERT max length ({MAX_LENGTH}): {sum(1 for length in text_lengths if length > MAX_LENGTH)}\")\n",
    "\n",
    "# Display all feature names\n",
    "print(f\"\\nAll Feature Columns ({len(dataset.feature_columns)} total):\")\n",
    "for i, col in enumerate(dataset.feature_columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "# Plot sequence length distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(sequence_lengths, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Sequence Length (time steps)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sequence Lengths (DGMs v3)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(text_lengths, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=MAX_LENGTH, color='red', linestyle='--', label=f'BERT Max Length ({MAX_LENGTH})')\n",
    "plt.xlabel('Text Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab243d78",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('dgms_bert_cv_results.csv', index=False)\n",
    "print(\"Results saved to 'dgms_bert_cv_results.csv'\")\n",
    "\n",
    "# Save detailed predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'true_label': all_true_labels,\n",
    "    'predicted_label': all_predictions,\n",
    "    'correct': np.array(all_true_labels) == np.array(all_predictions)\n",
    "})\n",
    "predictions_df.to_csv('dgms_bert_predictions.csv', index=False)\n",
    "print(\"Predictions saved to 'dgms_bert_predictions.csv'\")\n",
    "\n",
    "# Save feature list for reference\n",
    "feature_df = pd.DataFrame({\n",
    "    'feature_index': range(len(dataset.feature_columns)),\n",
    "    'feature_name': dataset.feature_columns\n",
    "})\n",
    "feature_df.to_csv('dgms_feature_list.csv', index=False)\n",
    "print(\"Feature list saved to 'dgms_feature_list.csv'\")\n",
    "\n",
    "print(\"\\nDGMs v3 BERT model training and evaluation completed successfully!\")\n",
    "print(f\"\\nKey differences from Raw Data approach:\")\n",
    "print(f\"- Used DGMs v3/3s processed data instead of Raw Data/all\")\n",
    "print(f\"- Trained on ALL {len(dataset.feature_columns)} features instead of 6 selected ones\")\n",
    "print(f\"- Adapted text representation for high-dimensional feature space\")\n",
    "print(f\"- Used feature chunking to manage BERT token limits\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
