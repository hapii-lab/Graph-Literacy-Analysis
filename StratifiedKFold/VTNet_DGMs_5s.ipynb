{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MANUAL_SEED = 1\n",
    "HIDDEN_SIZE = 256\n",
    "INPUT_SIZE = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533051b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA device(s):\n",
      "CUDA:0 - NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Found {num_devices} CUDA device(s):\")\n",
    "    for i in range(num_devices):\n",
    "        print(f\"CUDA:{i} - {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e5ce0",
   "metadata": {},
   "source": [
    "Here, a custom class is used to process the scanpath (.png) images and separate them into their respective classes (literate/illiterate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanpathDataset(Dataset):\n",
    "    def __init__(self, img_root, seq_root, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.label_map = {\"literate\": 0, \"illiterate\": 1}\n",
    "\n",
    "        for label_name, label in self.label_map.items():\n",
    "            img_dir = os.path.join(img_root, label_name)\n",
    "            seq_dir = os.path.join(seq_root, label_name)\n",
    "\n",
    "            for img_path in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "                base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "                csv_path = os.path.join(seq_dir, base + \".csv\")\n",
    "                if os.path.exists(csv_path):\n",
    "                    self.samples.append((img_path, csv_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, seq_path, label = self.samples[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        df = pd.read_csv(seq_path)\n",
    "\n",
    "        numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "        if numeric_df.empty:\n",
    "            raise ValueError(f\"No numeric columns found in {seq_path}!\")\n",
    "\n",
    "        seq = torch.tensor(numeric_df.values, dtype=torch.float32)\n",
    "\n",
    "        return img, seq, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, seqs, labels = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    seq_lengths = torch.tensor([len(seq) for seq in seqs])\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return images, padded_seqs, seq_lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e612949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VTNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size=50,\n",
    "                 rnn_hidden_size=256,\n",
    "                 output_size=2,\n",
    "                 rnn_type='gru',\n",
    "                 rnn_num_layers=1,\n",
    "                 n_channels_1=6,\n",
    "                 kernel_size_1=5,\n",
    "                 n_channels_2=16,\n",
    "                 kernel_size_2=5,\n",
    "                 img_n_vert=150,\n",
    "                 img_n_hor=150):\n",
    "        super(VTNet, self).__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "        self.n_channels_2 = n_channels_2\n",
    "\n",
    "        # --- CNN branch ---\n",
    "        self.conv1 = nn.Conv2d(1, n_channels_1, kernel_size=kernel_size_1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(n_channels_1, n_channels_2, kernel_size=kernel_size_2)\n",
    "\n",
    "        # calculate CNN output size\n",
    "        conv1_out_vert = img_n_vert - kernel_size_1 + 1\n",
    "        conv1_out_hor = img_n_hor - kernel_size_1 + 1\n",
    "        mp1_out_vert = (conv1_out_vert - 2) // 2 + 1\n",
    "        mp1_out_hor = (conv1_out_hor - 2) // 2 + 1\n",
    "        conv2_out_vert = mp1_out_vert - kernel_size_2 + 1\n",
    "        conv2_out_hor = mp1_out_hor - kernel_size_2 + 1\n",
    "        mp2_out_vert = (conv2_out_vert - 2) // 2 + 1\n",
    "        mp2_out_hor = (conv2_out_hor - 2) // 2 + 1\n",
    "\n",
    "        self.fc1 = nn.Linear(n_channels_2 * mp2_out_hor * mp2_out_vert, 50)\n",
    "        self.fc2 = nn.Linear(rnn_hidden_size + 50, 20)\n",
    "        # --- Attention projection layers ---\n",
    "        # project CNN features -> query, and RNN outputs -> keys/values\n",
    "        self.attn_query = nn.Linear(50, rnn_hidden_size)\n",
    "        self.attn_key = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.attn_value = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.fc3 = nn.Linear(20, output_size)\n",
    "\n",
    "        # --- RNN branch ---\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "                              num_layers=rnn_num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "                               num_layers=rnn_num_layers, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "                              num_layers=rnn_num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, scan_path, time_series, seq_lengths):\n",
    "        # --- CNN branch ---\n",
    "        x1 = self.pool(F.relu(self.conv1(scan_path)))\n",
    "        x1 = self.pool(F.relu(self.conv2(x1)))\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "\n",
    "        # --- RNN branch ---\n",
    "        packed = pack_padded_sequence(time_series, seq_lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.rnn(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        # grab last valid timestep for each sequence\n",
    "        idx = (seq_lengths - 1).view(-1, 1, 1).expand(-1, 1, out.size(2))\n",
    "        x2 = out.gather(1, idx).squeeze(1)\n",
    "\n",
    "        # --- combine CNN + RNN ---\n",
    "        # out: (B, T, H) where H = rnn_hidden_size\n",
    "        # We'll compute dot-product attention: query from CNN features, keys/values from RNN outputs\n",
    "        # Project query (from CNN) to rnn_hidden_size\n",
    "        # query: (B, H)\n",
    "        query = self.attn_query(x1)  # (B, H)\n",
    "\n",
    "        # keys, values: project RNN outputs\n",
    "        # keys/values: (B, T, H)\n",
    "        keys = self.attn_key(out)\n",
    "        values = self.attn_value(out)\n",
    "\n",
    "        # compute attention scores: (B, T)\n",
    "        scores = torch.bmm(query.unsqueeze(1), keys.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        # create mask for padded positions: (B, T)\n",
    "        max_len = out.size(1)\n",
    "        device = out.device\n",
    "        seq_range = torch.arange(0, max_len, device=device).unsqueeze(0)  # (1, T)\n",
    "        seq_lengths_exp = seq_lengths.unsqueeze(1)\n",
    "        mask = seq_range >= seq_lengths_exp  # True where padding\n",
    "\n",
    "        # apply mask: set scores at padding positions to large negative value\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=1)  # (B, T)\n",
    "\n",
    "        # weighted sum of values: (B, T, H) * (B, T, 1) -> (B, H)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), values).squeeze(1)\n",
    "\n",
    "        # --- combine CNN + attention-context ---\n",
    "        x = torch.cat((x1, context), 1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66ee9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1499\n",
      "\n",
      "========== Fold 1/5 ==========\n",
      "Detected input size: 50\n",
      "Epoch [1/50] | Train Loss: 0.6861 | Train Acc: 55.96%\n",
      "Epoch [2/50] | Train Loss: 0.6829 | Train Acc: 56.63%\n",
      "Epoch [3/50] | Train Loss: 0.6766 | Train Acc: 57.05%\n",
      "Epoch [4/50] | Train Loss: 0.6678 | Train Acc: 57.13%\n",
      "Epoch [5/50] | Train Loss: 0.6479 | Train Acc: 62.80%\n",
      "Epoch [6/50] | Train Loss: 0.6292 | Train Acc: 63.14%\n",
      "Epoch [7/50] | Train Loss: 0.6041 | Train Acc: 67.97%\n",
      "Epoch [8/50] | Train Loss: 0.5794 | Train Acc: 70.39%\n",
      "Epoch [9/50] | Train Loss: 0.5614 | Train Acc: 72.14%\n",
      "Epoch [10/50] | Train Loss: 0.5299 | Train Acc: 74.23%\n",
      "Epoch [11/50] | Train Loss: 0.5123 | Train Acc: 74.48%\n",
      "Epoch [12/50] | Train Loss: 0.4775 | Train Acc: 75.73%\n",
      "Epoch [13/50] | Train Loss: 0.4659 | Train Acc: 78.73%\n",
      "Epoch [14/50] | Train Loss: 0.4247 | Train Acc: 80.40%\n",
      "Epoch [15/50] | Train Loss: 0.4097 | Train Acc: 80.73%\n",
      "Epoch [16/50] | Train Loss: 0.3834 | Train Acc: 82.82%\n",
      "Epoch [17/50] | Train Loss: 0.3603 | Train Acc: 83.74%\n",
      "Epoch [18/50] | Train Loss: 0.3321 | Train Acc: 85.32%\n",
      "Epoch [19/50] | Train Loss: 0.3164 | Train Acc: 86.07%\n",
      "Epoch [20/50] | Train Loss: 0.2894 | Train Acc: 88.74%\n",
      "Epoch [21/50] | Train Loss: 0.2695 | Train Acc: 87.99%\n",
      "Epoch [22/50] | Train Loss: 0.2580 | Train Acc: 89.41%\n",
      "Epoch [23/50] | Train Loss: 0.2391 | Train Acc: 90.83%\n",
      "Epoch [24/50] | Train Loss: 0.2133 | Train Acc: 91.49%\n",
      "Epoch [25/50] | Train Loss: 0.1994 | Train Acc: 92.33%\n",
      "Epoch [26/50] | Train Loss: 0.1882 | Train Acc: 92.74%\n",
      "Epoch [27/50] | Train Loss: 0.1729 | Train Acc: 93.83%\n",
      "Epoch [28/50] | Train Loss: 0.1545 | Train Acc: 94.08%\n",
      "Epoch [29/50] | Train Loss: 0.1487 | Train Acc: 95.25%\n",
      "Epoch [30/50] | Train Loss: 0.1281 | Train Acc: 95.83%\n",
      "Epoch [31/50] | Train Loss: 0.1222 | Train Acc: 95.83%\n",
      "Epoch [32/50] | Train Loss: 0.1004 | Train Acc: 97.16%\n",
      "Epoch [33/50] | Train Loss: 0.0921 | Train Acc: 97.75%\n",
      "Epoch [34/50] | Train Loss: 0.0915 | Train Acc: 97.16%\n",
      "Epoch [35/50] | Train Loss: 0.0804 | Train Acc: 98.17%\n",
      "Epoch [36/50] | Train Loss: 0.0703 | Train Acc: 98.33%\n",
      "Epoch [37/50] | Train Loss: 0.0655 | Train Acc: 98.58%\n",
      "Epoch [38/50] | Train Loss: 0.0659 | Train Acc: 98.25%\n",
      "Epoch [39/50] | Train Loss: 0.0645 | Train Acc: 98.08%\n",
      "Epoch [40/50] | Train Loss: 0.0498 | Train Acc: 98.58%\n",
      "Epoch [41/50] | Train Loss: 0.0462 | Train Acc: 98.75%\n",
      "Epoch [42/50] | Train Loss: 0.0377 | Train Acc: 99.42%\n",
      "Epoch [43/50] | Train Loss: 0.0346 | Train Acc: 99.25%\n",
      "Epoch [44/50] | Train Loss: 0.0332 | Train Acc: 99.08%\n",
      "Epoch [45/50] | Train Loss: 0.0306 | Train Acc: 99.42%\n",
      "Epoch [46/50] | Train Loss: 0.0226 | Train Acc: 99.58%\n",
      "Epoch [47/50] | Train Loss: 0.0194 | Train Acc: 99.75%\n",
      "Epoch [48/50] | Train Loss: 0.0768 | Train Acc: 98.00%\n",
      "Epoch [49/50] | Train Loss: 0.0177 | Train Acc: 99.75%\n",
      "Epoch [50/50] | Train Loss: 0.0169 | Train Acc: 99.50%\n",
      "Fold 1 Results -> Acc: 0.827, Precision: 0.823, Recall: 0.824, F1: 0.824\n",
      "\n",
      "========== Fold 2/5 ==========\n",
      "Detected input size: 50\n",
      "Epoch [1/50] | Train Loss: 0.6905 | Train Acc: 53.79%\n",
      "Epoch [2/50] | Train Loss: 0.6804 | Train Acc: 56.30%\n",
      "Epoch [3/50] | Train Loss: 0.6794 | Train Acc: 57.13%\n",
      "Epoch [4/50] | Train Loss: 0.6786 | Train Acc: 55.46%\n",
      "Epoch [5/50] | Train Loss: 0.6660 | Train Acc: 58.30%\n",
      "Epoch [6/50] | Train Loss: 0.6414 | Train Acc: 61.22%\n",
      "Epoch [7/50] | Train Loss: 0.6141 | Train Acc: 65.72%\n",
      "Epoch [8/50] | Train Loss: 0.5813 | Train Acc: 70.98%\n",
      "Epoch [9/50] | Train Loss: 0.5568 | Train Acc: 71.98%\n",
      "Epoch [10/50] | Train Loss: 0.5324 | Train Acc: 73.89%\n",
      "Epoch [11/50] | Train Loss: 0.5091 | Train Acc: 75.81%\n",
      "Epoch [12/50] | Train Loss: 0.4907 | Train Acc: 75.73%\n",
      "Epoch [13/50] | Train Loss: 0.4639 | Train Acc: 78.90%\n",
      "Epoch [14/50] | Train Loss: 0.4404 | Train Acc: 79.73%\n",
      "Epoch [15/50] | Train Loss: 0.4165 | Train Acc: 81.07%\n",
      "Epoch [16/50] | Train Loss: 0.3903 | Train Acc: 82.65%\n",
      "Epoch [17/50] | Train Loss: 0.3851 | Train Acc: 83.32%\n",
      "Epoch [18/50] | Train Loss: 0.3576 | Train Acc: 83.74%\n",
      "Epoch [19/50] | Train Loss: 0.3357 | Train Acc: 86.57%\n",
      "Epoch [20/50] | Train Loss: 0.3175 | Train Acc: 86.16%\n",
      "Epoch [21/50] | Train Loss: 0.3073 | Train Acc: 86.99%\n",
      "Epoch [22/50] | Train Loss: 0.2820 | Train Acc: 88.74%\n",
      "Epoch [23/50] | Train Loss: 0.2682 | Train Acc: 88.91%\n",
      "Epoch [24/50] | Train Loss: 0.2605 | Train Acc: 89.74%\n",
      "Epoch [25/50] | Train Loss: 0.2515 | Train Acc: 90.33%\n",
      "Epoch [26/50] | Train Loss: 0.2347 | Train Acc: 90.08%\n",
      "Epoch [27/50] | Train Loss: 0.2095 | Train Acc: 91.91%\n",
      "Epoch [28/50] | Train Loss: 0.2065 | Train Acc: 91.41%\n",
      "Epoch [29/50] | Train Loss: 0.1940 | Train Acc: 92.41%\n",
      "Epoch [30/50] | Train Loss: 0.1781 | Train Acc: 93.49%\n",
      "Epoch [31/50] | Train Loss: 0.1729 | Train Acc: 93.33%\n",
      "Epoch [32/50] | Train Loss: 0.1605 | Train Acc: 93.66%\n",
      "Epoch [33/50] | Train Loss: 0.1399 | Train Acc: 95.00%\n",
      "Epoch [34/50] | Train Loss: 0.1390 | Train Acc: 94.91%\n",
      "Epoch [35/50] | Train Loss: 0.1274 | Train Acc: 95.16%\n",
      "Epoch [36/50] | Train Loss: 0.1187 | Train Acc: 95.83%\n",
      "Epoch [37/50] | Train Loss: 0.1039 | Train Acc: 96.33%\n",
      "Epoch [38/50] | Train Loss: 0.0998 | Train Acc: 96.66%\n",
      "Epoch [39/50] | Train Loss: 0.0907 | Train Acc: 97.00%\n",
      "Epoch [40/50] | Train Loss: 0.0813 | Train Acc: 97.00%\n",
      "Epoch [41/50] | Train Loss: 0.0838 | Train Acc: 97.41%\n",
      "Epoch [42/50] | Train Loss: 0.0677 | Train Acc: 97.66%\n",
      "Epoch [43/50] | Train Loss: 0.0707 | Train Acc: 97.58%\n",
      "Epoch [44/50] | Train Loss: 0.0596 | Train Acc: 98.25%\n",
      "Epoch [45/50] | Train Loss: 0.0518 | Train Acc: 98.58%\n",
      "Epoch [46/50] | Train Loss: 0.0428 | Train Acc: 99.17%\n",
      "Epoch [47/50] | Train Loss: 0.0526 | Train Acc: 98.33%\n",
      "Epoch [48/50] | Train Loss: 0.0521 | Train Acc: 98.25%\n",
      "Epoch [49/50] | Train Loss: 0.0465 | Train Acc: 98.92%\n",
      "Epoch [50/50] | Train Loss: 0.0286 | Train Acc: 99.67%\n",
      "Fold 2 Results -> Acc: 0.767, Precision: 0.763, Recall: 0.766, F1: 0.764\n",
      "\n",
      "========== Fold 3/5 ==========\n",
      "Detected input size: 50\n",
      "Epoch [1/50] | Train Loss: 0.6887 | Train Acc: 55.80%\n",
      "Epoch [2/50] | Train Loss: 0.6851 | Train Acc: 56.63%\n",
      "Epoch [3/50] | Train Loss: 0.6855 | Train Acc: 56.63%\n",
      "Epoch [4/50] | Train Loss: 0.6834 | Train Acc: 56.63%\n",
      "Epoch [5/50] | Train Loss: 0.6766 | Train Acc: 56.63%\n",
      "Epoch [6/50] | Train Loss: 0.6640 | Train Acc: 56.63%\n",
      "Epoch [7/50] | Train Loss: 0.6439 | Train Acc: 57.13%\n",
      "Epoch [8/50] | Train Loss: 0.6260 | Train Acc: 57.46%\n",
      "Epoch [9/50] | Train Loss: 0.6177 | Train Acc: 57.88%\n",
      "Epoch [10/50] | Train Loss: 0.6071 | Train Acc: 61.22%\n",
      "Epoch [11/50] | Train Loss: 0.5751 | Train Acc: 69.39%\n",
      "Epoch [12/50] | Train Loss: 0.5553 | Train Acc: 71.06%\n",
      "Epoch [13/50] | Train Loss: 0.5383 | Train Acc: 72.14%\n",
      "Epoch [14/50] | Train Loss: 0.5188 | Train Acc: 73.89%\n",
      "Epoch [15/50] | Train Loss: 0.5018 | Train Acc: 74.23%\n",
      "Epoch [16/50] | Train Loss: 0.4797 | Train Acc: 76.81%\n",
      "Epoch [17/50] | Train Loss: 0.4534 | Train Acc: 77.81%\n",
      "Epoch [18/50] | Train Loss: 0.4423 | Train Acc: 78.65%\n",
      "Epoch [19/50] | Train Loss: 0.4194 | Train Acc: 80.98%\n",
      "Epoch [20/50] | Train Loss: 0.4053 | Train Acc: 82.07%\n",
      "Epoch [21/50] | Train Loss: 0.3824 | Train Acc: 82.65%\n",
      "Epoch [22/50] | Train Loss: 0.3668 | Train Acc: 83.74%\n",
      "Epoch [23/50] | Train Loss: 0.3585 | Train Acc: 84.15%\n",
      "Epoch [24/50] | Train Loss: 0.3352 | Train Acc: 85.65%\n",
      "Epoch [25/50] | Train Loss: 0.3170 | Train Acc: 85.82%\n",
      "Epoch [26/50] | Train Loss: 0.2976 | Train Acc: 86.99%\n",
      "Epoch [27/50] | Train Loss: 0.2816 | Train Acc: 87.41%\n",
      "Epoch [28/50] | Train Loss: 0.2688 | Train Acc: 88.41%\n",
      "Epoch [29/50] | Train Loss: 0.2531 | Train Acc: 89.49%\n",
      "Epoch [30/50] | Train Loss: 0.2486 | Train Acc: 90.33%\n",
      "Epoch [31/50] | Train Loss: 0.2328 | Train Acc: 90.83%\n",
      "Epoch [32/50] | Train Loss: 0.2262 | Train Acc: 90.83%\n",
      "Epoch [33/50] | Train Loss: 0.2087 | Train Acc: 91.66%\n",
      "Epoch [34/50] | Train Loss: 0.1969 | Train Acc: 91.91%\n",
      "Epoch [35/50] | Train Loss: 0.1902 | Train Acc: 92.41%\n",
      "Epoch [36/50] | Train Loss: 0.1791 | Train Acc: 92.66%\n",
      "Epoch [37/50] | Train Loss: 0.1755 | Train Acc: 93.49%\n",
      "Epoch [38/50] | Train Loss: 0.1533 | Train Acc: 93.91%\n",
      "Epoch [39/50] | Train Loss: 0.1495 | Train Acc: 94.75%\n",
      "Epoch [40/50] | Train Loss: 0.1411 | Train Acc: 94.91%\n",
      "Epoch [41/50] | Train Loss: 0.1362 | Train Acc: 95.08%\n",
      "Epoch [42/50] | Train Loss: 0.1230 | Train Acc: 95.66%\n",
      "Epoch [43/50] | Train Loss: 0.1140 | Train Acc: 96.33%\n",
      "Epoch [44/50] | Train Loss: 0.1028 | Train Acc: 96.25%\n",
      "Epoch [45/50] | Train Loss: 0.1004 | Train Acc: 96.50%\n",
      "Epoch [46/50] | Train Loss: 0.0901 | Train Acc: 97.00%\n",
      "Epoch [47/50] | Train Loss: 0.0830 | Train Acc: 97.25%\n",
      "Epoch [48/50] | Train Loss: 0.0829 | Train Acc: 97.66%\n",
      "Epoch [49/50] | Train Loss: 0.0721 | Train Acc: 98.00%\n",
      "Epoch [50/50] | Train Loss: 0.0641 | Train Acc: 97.83%\n",
      "Fold 3 Results -> Acc: 0.797, Precision: 0.793, Recall: 0.794, F1: 0.794\n",
      "\n",
      "========== Fold 4/5 ==========\n",
      "Detected input size: 50\n",
      "Epoch [1/50] | Train Loss: 0.6860 | Train Acc: 56.55%\n",
      "Epoch [2/50] | Train Loss: 0.6839 | Train Acc: 56.71%\n",
      "Epoch [3/50] | Train Loss: 0.6812 | Train Acc: 56.88%\n",
      "Epoch [4/50] | Train Loss: 0.6765 | Train Acc: 56.88%\n",
      "Epoch [5/50] | Train Loss: 0.6733 | Train Acc: 57.38%\n",
      "Epoch [6/50] | Train Loss: 0.6645 | Train Acc: 58.05%\n",
      "Epoch [7/50] | Train Loss: 0.6516 | Train Acc: 60.30%\n",
      "Epoch [8/50] | Train Loss: 0.6226 | Train Acc: 64.30%\n",
      "Epoch [9/50] | Train Loss: 0.6024 | Train Acc: 65.97%\n",
      "Epoch [10/50] | Train Loss: 0.5819 | Train Acc: 68.14%\n",
      "Epoch [11/50] | Train Loss: 0.5677 | Train Acc: 70.39%\n",
      "Epoch [12/50] | Train Loss: 0.5472 | Train Acc: 72.98%\n",
      "Epoch [13/50] | Train Loss: 0.5208 | Train Acc: 73.81%\n",
      "Epoch [14/50] | Train Loss: 0.5141 | Train Acc: 74.56%\n",
      "Epoch [15/50] | Train Loss: 0.4867 | Train Acc: 76.40%\n",
      "Epoch [16/50] | Train Loss: 0.4602 | Train Acc: 77.98%\n",
      "Epoch [17/50] | Train Loss: 0.4400 | Train Acc: 78.82%\n",
      "Epoch [18/50] | Train Loss: 0.4305 | Train Acc: 79.98%\n",
      "Epoch [19/50] | Train Loss: 0.3974 | Train Acc: 81.73%\n",
      "Epoch [20/50] | Train Loss: 0.3718 | Train Acc: 83.07%\n",
      "Epoch [21/50] | Train Loss: 0.3464 | Train Acc: 84.90%\n",
      "Epoch [22/50] | Train Loss: 0.3161 | Train Acc: 86.41%\n",
      "Epoch [23/50] | Train Loss: 0.2971 | Train Acc: 87.82%\n",
      "Epoch [24/50] | Train Loss: 0.2776 | Train Acc: 88.24%\n",
      "Epoch [25/50] | Train Loss: 0.2582 | Train Acc: 89.07%\n",
      "Epoch [26/50] | Train Loss: 0.2326 | Train Acc: 90.83%\n",
      "Epoch [27/50] | Train Loss: 0.2149 | Train Acc: 91.58%\n",
      "Epoch [28/50] | Train Loss: 0.2005 | Train Acc: 92.33%\n",
      "Epoch [29/50] | Train Loss: 0.1977 | Train Acc: 92.08%\n",
      "Epoch [30/50] | Train Loss: 0.1777 | Train Acc: 93.08%\n",
      "Epoch [31/50] | Train Loss: 0.1568 | Train Acc: 94.50%\n",
      "Epoch [32/50] | Train Loss: 0.1471 | Train Acc: 94.41%\n",
      "Epoch [33/50] | Train Loss: 0.1326 | Train Acc: 95.75%\n",
      "Epoch [34/50] | Train Loss: 0.1176 | Train Acc: 96.41%\n",
      "Epoch [35/50] | Train Loss: 0.1167 | Train Acc: 95.66%\n",
      "Epoch [36/50] | Train Loss: 0.0971 | Train Acc: 97.33%\n",
      "Epoch [37/50] | Train Loss: 0.0881 | Train Acc: 96.83%\n",
      "Epoch [38/50] | Train Loss: 0.0815 | Train Acc: 97.33%\n",
      "Epoch [39/50] | Train Loss: 0.0756 | Train Acc: 97.66%\n",
      "Epoch [40/50] | Train Loss: 0.0612 | Train Acc: 97.91%\n",
      "Epoch [41/50] | Train Loss: 0.0702 | Train Acc: 97.83%\n",
      "Epoch [42/50] | Train Loss: 0.0522 | Train Acc: 98.50%\n",
      "Epoch [43/50] | Train Loss: 0.0405 | Train Acc: 99.17%\n",
      "Epoch [44/50] | Train Loss: 0.0389 | Train Acc: 99.17%\n",
      "Epoch [45/50] | Train Loss: 0.0282 | Train Acc: 99.83%\n",
      "Epoch [46/50] | Train Loss: 0.0309 | Train Acc: 99.17%\n",
      "Epoch [47/50] | Train Loss: 0.0258 | Train Acc: 99.75%\n",
      "Epoch [48/50] | Train Loss: 0.0196 | Train Acc: 99.83%\n",
      "Epoch [49/50] | Train Loss: 0.0167 | Train Acc: 99.92%\n",
      "Epoch [50/50] | Train Loss: 0.0255 | Train Acc: 98.92%\n",
      "Fold 4 Results -> Acc: 0.807, Precision: 0.803, Recall: 0.807, F1: 0.804\n",
      "\n",
      "========== Fold 5/5 ==========\n",
      "Detected input size: 50\n",
      "Epoch [1/50] | Train Loss: 0.6880 | Train Acc: 54.92%\n",
      "Epoch [2/50] | Train Loss: 0.6814 | Train Acc: 56.42%\n",
      "Epoch [3/50] | Train Loss: 0.6718 | Train Acc: 57.92%\n",
      "Epoch [4/50] | Train Loss: 0.6433 | Train Acc: 60.58%\n",
      "Epoch [5/50] | Train Loss: 0.6117 | Train Acc: 66.92%\n",
      "Epoch [6/50] | Train Loss: 0.5738 | Train Acc: 71.75%\n",
      "Epoch [7/50] | Train Loss: 0.5424 | Train Acc: 73.92%\n",
      "Epoch [8/50] | Train Loss: 0.5093 | Train Acc: 73.50%\n",
      "Epoch [9/50] | Train Loss: 0.4767 | Train Acc: 77.08%\n",
      "Epoch [10/50] | Train Loss: 0.4330 | Train Acc: 79.58%\n",
      "Epoch [11/50] | Train Loss: 0.3974 | Train Acc: 82.17%\n",
      "Epoch [12/50] | Train Loss: 0.3567 | Train Acc: 83.42%\n",
      "Epoch [13/50] | Train Loss: 0.3290 | Train Acc: 85.67%\n",
      "Epoch [14/50] | Train Loss: 0.2946 | Train Acc: 87.08%\n",
      "Epoch [15/50] | Train Loss: 0.2686 | Train Acc: 88.75%\n",
      "Epoch [16/50] | Train Loss: 0.2414 | Train Acc: 89.83%\n",
      "Epoch [17/50] | Train Loss: 0.2255 | Train Acc: 90.42%\n",
      "Epoch [18/50] | Train Loss: 0.1986 | Train Acc: 91.83%\n",
      "Epoch [19/50] | Train Loss: 0.1779 | Train Acc: 92.50%\n",
      "Epoch [20/50] | Train Loss: 0.1625 | Train Acc: 93.58%\n",
      "Epoch [21/50] | Train Loss: 0.1456 | Train Acc: 94.58%\n",
      "Epoch [22/50] | Train Loss: 0.1260 | Train Acc: 95.58%\n",
      "Epoch [23/50] | Train Loss: 0.1063 | Train Acc: 96.25%\n",
      "Epoch [24/50] | Train Loss: 0.0956 | Train Acc: 96.50%\n",
      "Epoch [25/50] | Train Loss: 0.0926 | Train Acc: 97.33%\n",
      "Epoch [26/50] | Train Loss: 0.0763 | Train Acc: 97.83%\n",
      "Epoch [27/50] | Train Loss: 0.0694 | Train Acc: 97.67%\n",
      "Epoch [28/50] | Train Loss: 0.0595 | Train Acc: 98.58%\n",
      "Epoch [29/50] | Train Loss: 0.0498 | Train Acc: 98.92%\n",
      "Epoch [30/50] | Train Loss: 0.0390 | Train Acc: 99.50%\n",
      "Epoch [31/50] | Train Loss: 0.0353 | Train Acc: 99.33%\n",
      "Epoch [32/50] | Train Loss: 0.0284 | Train Acc: 99.67%\n",
      "Epoch [33/50] | Train Loss: 0.0227 | Train Acc: 99.83%\n",
      "Epoch [34/50] | Train Loss: 0.0207 | Train Acc: 99.83%\n",
      "Epoch [35/50] | Train Loss: 0.0175 | Train Acc: 99.92%\n",
      "Epoch [36/50] | Train Loss: 0.0152 | Train Acc: 99.92%\n",
      "Epoch [37/50] | Train Loss: 0.0122 | Train Acc: 99.92%\n",
      "Epoch [38/50] | Train Loss: 0.0088 | Train Acc: 100.00%\n",
      "Epoch [39/50] | Train Loss: 0.0073 | Train Acc: 100.00%\n",
      "Epoch [40/50] | Train Loss: 0.0061 | Train Acc: 100.00%\n",
      "Epoch [41/50] | Train Loss: 0.0045 | Train Acc: 100.00%\n",
      "Epoch [42/50] | Train Loss: 0.0047 | Train Acc: 100.00%\n",
      "Epoch [43/50] | Train Loss: 0.0034 | Train Acc: 100.00%\n",
      "Epoch [44/50] | Train Loss: 0.0025 | Train Acc: 100.00%\n",
      "Epoch [45/50] | Train Loss: 0.0028 | Train Acc: 100.00%\n",
      "Epoch [46/50] | Train Loss: 0.0025 | Train Acc: 100.00%\n",
      "Epoch [47/50] | Train Loss: 0.0022 | Train Acc: 100.00%\n",
      "Epoch [48/50] | Train Loss: 0.0017 | Train Acc: 100.00%\n",
      "Epoch [49/50] | Train Loss: 0.0010 | Train Acc: 100.00%\n",
      "Epoch [50/50] | Train Loss: 0.0007 | Train Acc: 100.00%\n",
      "Fold 5 Results -> Acc: 0.789, Precision: 0.788, Recall: 0.781, F1: 0.783\n",
      "\n",
      "========== Cross-Validation Results ==========\n",
      "Accuracy:  0.797 ± 0.020\n",
      "Precision: 0.794 ± 0.020\n",
      "Recall:    0.794 ± 0.020\n",
      "F1-score:  0.794 ± 0.020\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ScanpathDataset(\"Scanpaths/all\", \"DGMs v3/5s\", transform=transform)\n",
    "print(\"Total samples:\", len(dataset))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Get all labels from dataset for stratification\n",
    "# ------------------------------------------------\n",
    "all_labels = [dataset[i][2] for i in range(len(dataset))]  # assuming (image, seq, label)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Stratified K-Fold setup\n",
    "# ------------------------------------------------\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(all_labels)), all_labels)):\n",
    "    print(f\"\\n========== Fold {fold+1}/{num_folds} ==========\")\n",
    "    \n",
    "    # Create DataLoaders for this fold\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_subset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # infer input_size from one CSV\n",
    "    sample_img, sample_seq, _ = dataset[0]\n",
    "    input_size = sample_seq.shape[1]\n",
    "    print(\"Detected input size:\", input_size)\n",
    "\n",
    "    # Re-initialize the model for each fold\n",
    "    model = VTNet(input_size=input_size, rnn_type='gru').to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training\n",
    "    # -----------------------------\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, sequences, seq_lengths, labels in train_loader:\n",
    "            images, sequences, seq_lengths, labels = (\n",
    "                images.to(device),\n",
    "                sequences.to(device),\n",
    "                seq_lengths.to(device),\n",
    "                labels.to(device)\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, sequences, seq_lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # --- optional: print intermediate training progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Validation\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, sequences, seq_lengths, labels in val_loader:\n",
    "            images, sequences, seq_lengths, labels = (\n",
    "                images.to(device),\n",
    "                sequences.to(device),\n",
    "                seq_lengths.to(device),\n",
    "                labels.to(device)\n",
    "            )\n",
    "            outputs = model(images, sequences, seq_lengths)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics for this fold\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    precision = precision_score(all_true, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_true, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_true, all_preds, average=\"macro\")\n",
    "\n",
    "    fold_metrics.append((acc, precision, recall, f1))\n",
    "    print(f\"Fold {fold+1} Results -> Acc: {acc:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Average metrics across folds\n",
    "# ------------------------------------------------\n",
    "fold_metrics = np.array(fold_metrics)\n",
    "mean_metrics = fold_metrics.mean(axis=0)\n",
    "std_metrics = fold_metrics.std(axis=0)\n",
    "\n",
    "print(\"\\n========== Cross-Validation Results ==========\")\n",
    "print(f\"Accuracy:  {mean_metrics[0]:.3f} ± {std_metrics[0]:.3f}\")\n",
    "print(f\"Precision: {mean_metrics[1]:.3f} ± {std_metrics[1]:.3f}\")\n",
    "print(f\"Recall:    {mean_metrics[2]:.3f} ± {std_metrics[2]:.3f}\")\n",
    "print(f\"F1-score:  {mean_metrics[3]:.3f} ± {std_metrics[3]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163081db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size \u001b[38;5;241m-\u001b[39m val_size\n\u001b[0;32m     15\u001b[0m train_ds, val_ds, test_ds \u001b[38;5;241m=\u001b[39m random_split(dataset, [train_size, val_size, test_size])\n\u001b[1;32m---> 17\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m     18\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m     19\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n",
      "File \u001b[1;32mc:\\Users\\hapii4u\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 388\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hapii4u\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:156\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ScanpathDataset(\"Scanpaths/contaminated\", \"DGMs v2/3s/all\", transform=transform)\n",
    "print(\"Total samples:\", len(dataset))\n",
    "\n",
    "# train/val/test split (70/15/15)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# infer input_size from one CSV\n",
    "sample_img, sample_seq, _ = dataset[0]\n",
    "input_size = sample_seq.shape[1]\n",
    "print(\"Detected input size:\", input_size)\n",
    "\n",
    "# -----------------------------\n",
    "# Model + training setup\n",
    "# -----------------------------\n",
    "model = VTNet(input_size=input_size, rnn_type='gru').to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# Training + Validation\n",
    "# -----------------------------\n",
    "for epoch in range(50):\n",
    "    # --- training ---\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for images, sequences, seq_lengths, labels in train_loader:\n",
    "        images, sequences, seq_lengths, labels = (\n",
    "            images.to(device),\n",
    "            sequences.to(device),\n",
    "            seq_lengths.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, sequences, seq_lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total * 100\n",
    "\n",
    "    # --- validation ---\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, sequences, seq_lengths, labels in val_loader:\n",
    "            images, sequences, seq_lengths, labels = (\n",
    "                images.to(device),\n",
    "                sequences.to(device),\n",
    "                seq_lengths.to(device),\n",
    "                labels.to(device)\n",
    "            )\n",
    "            outputs = model(images, sequences, seq_lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_acc = val_correct / val_total * 100\n",
    "    val_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    val_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | \"\n",
    "          f\"Train Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.2f}% | \"\n",
    "          f\"P: {val_precision:.3f}, R: {val_recall:.3f}, F1: {val_f1:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final Test\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "test_correct, test_total = 0, 0\n",
    "all_labels, all_preds = [], []\n",
    "with torch.no_grad():\n",
    "    for images, sequences, seq_lengths, labels in test_loader:\n",
    "        images, sequences, seq_lengths, labels = (\n",
    "            images.to(device),\n",
    "            sequences.to(device),\n",
    "            seq_lengths.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "        outputs = model(images, sequences, seq_lengths)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "test_acc = test_correct / test_total * 100\n",
    "test_precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "test_recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "test_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\n--- Final Test Results ---\")\n",
    "print(f\"Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Precision: {test_precision:.3f}\")\n",
    "print(f\"Recall: {test_recall:.3f}\")\n",
    "print(f\"F1-score: {test_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09a013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    literate       0.80      0.66      0.73        98\n",
      "  illiterate       0.77      0.88      0.82       128\n",
      "\n",
      "    accuracy                           0.78       226\n",
      "   macro avg       0.79      0.77      0.77       226\n",
      "weighted avg       0.79      0.78      0.78       226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# --- Collect predictions and labels ---\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, sequences, seq_lengths, labels in test_loader:\n",
    "        images, sequences, seq_lengths, labels = (\n",
    "            images.to(device),\n",
    "            sequences.to(device),\n",
    "            seq_lengths.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "\n",
    "        outputs = model(images, sequences, seq_lengths)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "classes = [\"literate\", \"illiterate\"]  # adjust if reversed\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(classification_report(y_true, y_pred, target_names=classes, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2b7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m input_size = \u001b[32m2\u001b[39m   \u001b[38;5;66;03m# features per timestep\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# generate dummy inputs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m scan_path = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m time_series = torch.randn(batch_size, seq_len, input_size, device=device, dtype=torch.float32)\n\u001b[32m      8\u001b[39m seq_lengths = torch.tensor([seq_len] * batch_size, dtype=torch.long, device=device)  \u001b[38;5;66;03m# all full-length sequences\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "seq_len = 30     # number of timesteps in your series\n",
    "input_size = 2   # features per timestep\n",
    "\n",
    "# generate dummy inputs\n",
    "scan_path = torch.randn(batch_size, 1, 150, 150, device=device, dtype=torch.float32)\n",
    "time_series = torch.randn(batch_size, seq_len, input_size, device=device, dtype=torch.float32)\n",
    "seq_lengths = torch.tensor([seq_len] * batch_size, dtype=torch.long, device=device)  # all full-length sequences\n",
    "\n",
    "# run forward pass (no hidden needed)\n",
    "output = model(scan_path, time_series, seq_lengths)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output logits:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a594e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=18496, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=306, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (rnn): GRU(2, 256, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f782cb",
   "metadata": {},
   "source": [
    "Below was a function used to rename the scanpath files. It is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67caeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Base folder\n",
    "base_folder = \"Scanpaths/contaminated\"\n",
    "subfolders = [\"literate\", \"illiterate\"]\n",
    "\n",
    "for sub in subfolders:\n",
    "    folder_path = os.path.join(base_folder, sub)\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\") and \"_scanpath\" in filename:\n",
    "            old_path = os.path.join(folder_path, filename)\n",
    "            new_filename = filename.replace(\"_scanpath\", \"\")\n",
    "            new_path = os.path.join(folder_path, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed: {filename} -> {new_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
