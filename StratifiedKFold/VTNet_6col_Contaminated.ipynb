{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MANUAL_SEED = 1\n",
    "HIDDEN_SIZE = 256\n",
    "INPUT_SIZE = 6  # FPOGX, FPOGY, LPS, RPS, LPMM, RPMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "409238c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA device(s):\n",
      "CUDA:0 - NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Found {num_devices} CUDA device(s):\")\n",
    "    for i in range(num_devices):\n",
    "        print(f\"CUDA:{i} - {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60f343",
   "metadata": {},
   "source": [
    "Here, a custom class is used to process the scanpath (.png) images and separate them into their respective classes (literate/illiterate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7557d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanpathDataset(Dataset):\n",
    "    def __init__(self, img_root, seq_root, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.label_map = {\"literate\": 0, \"illiterate\": 1}\n",
    "\n",
    "        for label_name, label in self.label_map.items():\n",
    "            img_dir = os.path.join(img_root, label_name)\n",
    "            seq_dir = os.path.join(seq_root, label_name)\n",
    "\n",
    "            for img_path in glob.glob(os.path.join(img_dir, \"*.png\")):\n",
    "                base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "                csv_path = os.path.join(seq_dir, base + \".csv\")\n",
    "                if os.path.exists(csv_path):\n",
    "                    self.samples.append((img_path, csv_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        from PIL import Image\n",
    "        img_path, seq_path, label = self.samples[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        df = pd.read_csv(seq_path)\n",
    "        \n",
    "        required_columns = ['FPOGX', 'FPOGY', 'LPS', 'RPS', 'LPMM', 'RPMM']\n",
    "        \n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"CSV file {seq_path} missing columns: {missing_cols}\")\n",
    "\n",
    "        seq = df[required_columns].values\n",
    "        seq = torch.tensor(seq, dtype=torch.float32)\n",
    "\n",
    "        return img, seq, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a99838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, seqs, labels = zip(*batch)\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    seq_lengths = torch.tensor([len(seq) for seq in seqs])\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return images, padded_seqs, seq_lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13082a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VTNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size=6,\n",
    "                 rnn_hidden_size=256,\n",
    "                 output_size=2,\n",
    "                 rnn_type='gru',\n",
    "                 rnn_num_layers=1,\n",
    "                 n_channels_1=6,\n",
    "                 kernel_size_1=5,\n",
    "                 n_channels_2=16,\n",
    "                 kernel_size_2=5,\n",
    "                 img_n_vert=150,\n",
    "                 img_n_hor=150):\n",
    "        super(VTNet, self).__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "        self.n_channels_2 = n_channels_2\n",
    "\n",
    "        # --- CNN branch ---\n",
    "        self.conv1 = nn.Conv2d(1, n_channels_1, kernel_size=kernel_size_1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(n_channels_1, n_channels_2, kernel_size=kernel_size_2)\n",
    "\n",
    "        # calculate CNN output size\n",
    "        conv1_out_vert = img_n_vert - kernel_size_1 + 1\n",
    "        conv1_out_hor = img_n_hor - kernel_size_1 + 1\n",
    "        mp1_out_vert = (conv1_out_vert - 2) // 2 + 1\n",
    "        mp1_out_hor = (conv1_out_hor - 2) // 2 + 1\n",
    "        conv2_out_vert = mp1_out_vert - kernel_size_2 + 1\n",
    "        conv2_out_hor = mp1_out_hor - kernel_size_2 + 1\n",
    "        mp2_out_vert = (conv2_out_vert - 2) // 2 + 1\n",
    "        mp2_out_hor = (conv2_out_hor - 2) // 2 + 1\n",
    "\n",
    "        self.fc1 = nn.Linear(n_channels_2 * mp2_out_hor * mp2_out_vert, 50)\n",
    "        self.fc2 = nn.Linear(rnn_hidden_size + 50, 20)\n",
    "        # --- Attention projection layers ---\n",
    "        # project CNN features -> query, and RNN outputs -> keys/values\n",
    "        self.attn_query = nn.Linear(50, rnn_hidden_size)\n",
    "        self.attn_key = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.attn_value = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.fc3 = nn.Linear(20, output_size)\n",
    "\n",
    "        # --- RNN branch ---\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "                              num_layers=rnn_num_layers, batch_first=True)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "                               num_layers=rnn_num_layers, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size=input_size, hidden_size=rnn_hidden_size,\n",
    "                              num_layers=rnn_num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, scan_path, time_series, seq_lengths):\n",
    "        # --- CNN branch ---\n",
    "        x1 = self.pool(F.relu(self.conv1(scan_path)))\n",
    "        x1 = self.pool(F.relu(self.conv2(x1)))\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x1 = F.relu(self.fc1(x1))\n",
    "\n",
    "        # --- RNN branch ---\n",
    "        packed = pack_padded_sequence(time_series, seq_lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.rnn(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        # --- Attention mechanism ---\n",
    "        # out: (B, T, H) where H = rnn_hidden_size\n",
    "        # We'll compute dot-product attention: query from CNN features, keys/values from RNN outputs\n",
    "        # Project query (from CNN) to rnn_hidden_size\n",
    "        # query: (B, H)\n",
    "        query = self.attn_query(x1)  # (B, H)\n",
    "\n",
    "        # keys, values: project RNN outputs\n",
    "        # keys/values: (B, T, H)\n",
    "        keys = self.attn_key(out)\n",
    "        values = self.attn_value(out)\n",
    "\n",
    "        # compute attention scores: (B, T)\n",
    "        scores = torch.bmm(query.unsqueeze(1), keys.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        # create mask for padded positions: (B, T)\n",
    "        max_len = out.size(1)\n",
    "        device = out.device\n",
    "        seq_range = torch.arange(0, max_len, device=device).unsqueeze(0)  # (1, T)\n",
    "        seq_lengths_exp = seq_lengths.unsqueeze(1)\n",
    "        mask = seq_range >= seq_lengths_exp  # True where padding\n",
    "\n",
    "        # apply mask: set scores at padding positions to large negative value\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=1)  # (B, T)\n",
    "\n",
    "        # weighted sum of values: (B, T, H) * (B, T, 1) -> (B, H)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), values).squeeze(1)\n",
    "\n",
    "        # --- combine CNN + attention-context ---\n",
    "        x = torch.cat((x1, context), 1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0fe02",
   "metadata": {},
   "source": [
    "## VTNet Model with Attention Mechanism\n",
    "\n",
    "This model combines:\n",
    "- **CNN branch**: Extracts spatial features from scanpath images (visual patterns)\n",
    "- **RNN branch**: Processes temporal sequences of eye movements (FPOGX, FPOGY, LPS, RPS, LPMM, RPMM)\n",
    "- **Attention mechanism**: Uses CNN features as query to selectively attend to important timesteps in the RNN sequence\n",
    "\n",
    "The attention allows the model to identify which temporal patterns in eye movements are most relevant given the overall spatial scanpath pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f305f",
   "metadata": {},
   "source": [
    "## Training Configuration 1: Cross-Validation on Full Dataset\n",
    "\n",
    "This section performs 10-fold stratified cross-validation on the complete dataset from **Scanpaths/all** and **Raw Data/all**. This provides robust performance estimates across different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f6c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1499\n",
      "\n",
      "========== Fold 1/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6838 | Train Acc: 56.63%\n",
      "Epoch [2/50] | Train Loss: 0.6546 | Train Acc: 59.15%\n",
      "Epoch [3/50] | Train Loss: 0.5837 | Train Acc: 68.87%\n",
      "Epoch [4/50] | Train Loss: 0.5422 | Train Acc: 71.68%\n",
      "Epoch [5/50] | Train Loss: 0.4735 | Train Acc: 76.72%\n",
      "Epoch [6/50] | Train Loss: 0.4250 | Train Acc: 79.54%\n",
      "Epoch [7/50] | Train Loss: 0.3743 | Train Acc: 82.21%\n",
      "Epoch [8/50] | Train Loss: 0.3256 | Train Acc: 85.99%\n",
      "Epoch [9/50] | Train Loss: 0.2897 | Train Acc: 87.25%\n",
      "Epoch [10/50] | Train Loss: 0.2535 | Train Acc: 89.18%\n",
      "Epoch [11/50] | Train Loss: 0.2185 | Train Acc: 91.40%\n",
      "Epoch [12/50] | Train Loss: 0.2331 | Train Acc: 89.84%\n",
      "Epoch [13/50] | Train Loss: 0.1768 | Train Acc: 92.36%\n",
      "Epoch [14/50] | Train Loss: 0.1523 | Train Acc: 94.29%\n",
      "Epoch [15/50] | Train Loss: 0.1504 | Train Acc: 93.85%\n",
      "Epoch [16/50] | Train Loss: 0.1306 | Train Acc: 95.03%\n",
      "Epoch [17/50] | Train Loss: 0.1180 | Train Acc: 95.63%\n",
      "Epoch [18/50] | Train Loss: 0.1171 | Train Acc: 96.07%\n",
      "Epoch [19/50] | Train Loss: 0.0869 | Train Acc: 96.81%\n",
      "Epoch [20/50] | Train Loss: 0.0765 | Train Acc: 97.78%\n",
      "Epoch [21/50] | Train Loss: 0.0943 | Train Acc: 96.44%\n",
      "Epoch [22/50] | Train Loss: 0.0727 | Train Acc: 97.55%\n",
      "Epoch [23/50] | Train Loss: 0.0588 | Train Acc: 98.30%\n",
      "Epoch [24/50] | Train Loss: 0.0421 | Train Acc: 98.67%\n",
      "Epoch [25/50] | Train Loss: 0.0394 | Train Acc: 98.52%\n",
      "Epoch [26/50] | Train Loss: 0.0427 | Train Acc: 98.74%\n",
      "Epoch [27/50] | Train Loss: 0.0411 | Train Acc: 98.74%\n",
      "Epoch [28/50] | Train Loss: 0.0592 | Train Acc: 98.37%\n",
      "Epoch [29/50] | Train Loss: 0.0350 | Train Acc: 98.81%\n",
      "Epoch [30/50] | Train Loss: 0.0153 | Train Acc: 99.70%\n",
      "Epoch [31/50] | Train Loss: 0.0178 | Train Acc: 99.70%\n",
      "Epoch [32/50] | Train Loss: 0.0303 | Train Acc: 99.18%\n",
      "Epoch [33/50] | Train Loss: 0.0234 | Train Acc: 99.26%\n",
      "Epoch [34/50] | Train Loss: 0.0088 | Train Acc: 99.85%\n",
      "Epoch [35/50] | Train Loss: 0.0584 | Train Acc: 97.92%\n",
      "Epoch [36/50] | Train Loss: 0.0248 | Train Acc: 99.18%\n",
      "Epoch [37/50] | Train Loss: 0.0206 | Train Acc: 99.41%\n",
      "Epoch [38/50] | Train Loss: 0.0028 | Train Acc: 100.00%\n",
      "Epoch [39/50] | Train Loss: 0.0012 | Train Acc: 100.00%\n",
      "Epoch [40/50] | Train Loss: 0.0010 | Train Acc: 100.00%\n",
      "Epoch [41/50] | Train Loss: 0.0007 | Train Acc: 100.00%\n",
      "Epoch [42/50] | Train Loss: 0.0004 | Train Acc: 100.00%\n",
      "Epoch [43/50] | Train Loss: 0.0003 | Train Acc: 100.00%\n",
      "Epoch [44/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [45/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [46/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [47/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [48/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [49/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [50/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Fold 1 Results -> Acc: 0.900, Precision: 0.898, Recall: 0.899, F1: 0.898\n",
      "\n",
      "========== Fold 2/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6806 | Train Acc: 55.67%\n",
      "Epoch [2/50] | Train Loss: 0.6705 | Train Acc: 57.52%\n",
      "Epoch [3/50] | Train Loss: 0.6349 | Train Acc: 66.05%\n",
      "Epoch [4/50] | Train Loss: 0.5945 | Train Acc: 69.76%\n",
      "Epoch [5/50] | Train Loss: 0.5539 | Train Acc: 71.83%\n",
      "Epoch [6/50] | Train Loss: 0.5374 | Train Acc: 73.24%\n",
      "Epoch [7/50] | Train Loss: 0.5105 | Train Acc: 75.39%\n",
      "Epoch [8/50] | Train Loss: 0.4726 | Train Acc: 77.39%\n",
      "Epoch [9/50] | Train Loss: 0.4315 | Train Acc: 79.76%\n",
      "Epoch [10/50] | Train Loss: 0.4109 | Train Acc: 81.10%\n",
      "Epoch [11/50] | Train Loss: 0.3770 | Train Acc: 82.65%\n",
      "Epoch [12/50] | Train Loss: 0.3650 | Train Acc: 83.62%\n",
      "Epoch [13/50] | Train Loss: 0.3415 | Train Acc: 84.21%\n",
      "Epoch [14/50] | Train Loss: 0.3043 | Train Acc: 86.88%\n",
      "Epoch [15/50] | Train Loss: 0.2824 | Train Acc: 87.77%\n",
      "Epoch [16/50] | Train Loss: 0.2666 | Train Acc: 88.88%\n",
      "Epoch [17/50] | Train Loss: 0.2413 | Train Acc: 90.07%\n",
      "Epoch [18/50] | Train Loss: 0.2166 | Train Acc: 90.96%\n",
      "Epoch [19/50] | Train Loss: 0.2513 | Train Acc: 90.29%\n",
      "Epoch [20/50] | Train Loss: 0.1948 | Train Acc: 92.14%\n",
      "Epoch [21/50] | Train Loss: 0.1779 | Train Acc: 93.03%\n",
      "Epoch [22/50] | Train Loss: 0.1706 | Train Acc: 93.11%\n",
      "Epoch [23/50] | Train Loss: 0.1592 | Train Acc: 94.14%\n",
      "Epoch [24/50] | Train Loss: 0.1521 | Train Acc: 93.85%\n",
      "Epoch [25/50] | Train Loss: 0.1433 | Train Acc: 94.81%\n",
      "Epoch [26/50] | Train Loss: 0.1151 | Train Acc: 95.70%\n",
      "Epoch [27/50] | Train Loss: 0.1212 | Train Acc: 95.55%\n",
      "Epoch [28/50] | Train Loss: 0.1019 | Train Acc: 96.15%\n",
      "Epoch [29/50] | Train Loss: 0.1149 | Train Acc: 95.70%\n",
      "Epoch [30/50] | Train Loss: 0.0743 | Train Acc: 97.11%\n",
      "Epoch [31/50] | Train Loss: 0.0804 | Train Acc: 97.26%\n",
      "Epoch [32/50] | Train Loss: 0.0832 | Train Acc: 97.18%\n",
      "Epoch [33/50] | Train Loss: 0.0657 | Train Acc: 97.78%\n",
      "Epoch [34/50] | Train Loss: 0.0531 | Train Acc: 98.15%\n",
      "Epoch [35/50] | Train Loss: 0.0732 | Train Acc: 97.85%\n",
      "Epoch [36/50] | Train Loss: 0.0420 | Train Acc: 98.96%\n",
      "Epoch [37/50] | Train Loss: 0.0239 | Train Acc: 99.41%\n",
      "Epoch [38/50] | Train Loss: 0.0362 | Train Acc: 98.81%\n",
      "Epoch [39/50] | Train Loss: 0.0438 | Train Acc: 98.59%\n",
      "Epoch [40/50] | Train Loss: 0.0528 | Train Acc: 98.15%\n",
      "Epoch [41/50] | Train Loss: 0.0235 | Train Acc: 99.33%\n",
      "Epoch [42/50] | Train Loss: 0.0238 | Train Acc: 99.18%\n",
      "Epoch [43/50] | Train Loss: 0.0744 | Train Acc: 98.22%\n",
      "Epoch [44/50] | Train Loss: 0.0130 | Train Acc: 99.78%\n",
      "Epoch [45/50] | Train Loss: 0.0382 | Train Acc: 99.04%\n",
      "Epoch [46/50] | Train Loss: 0.0695 | Train Acc: 97.78%\n",
      "Epoch [47/50] | Train Loss: 0.0208 | Train Acc: 99.63%\n",
      "Epoch [48/50] | Train Loss: 0.0078 | Train Acc: 99.85%\n",
      "Epoch [49/50] | Train Loss: 0.0053 | Train Acc: 99.93%\n",
      "Epoch [50/50] | Train Loss: 0.0026 | Train Acc: 100.00%\n",
      "Fold 2 Results -> Acc: 0.913, Precision: 0.911, Recall: 0.913, F1: 0.912\n",
      "\n",
      "========== Fold 3/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6843 | Train Acc: 55.89%\n",
      "Epoch [2/50] | Train Loss: 0.6684 | Train Acc: 57.45%\n",
      "Epoch [3/50] | Train Loss: 0.6205 | Train Acc: 66.42%\n",
      "Epoch [4/50] | Train Loss: 0.5621 | Train Acc: 71.24%\n",
      "Epoch [5/50] | Train Loss: 0.4969 | Train Acc: 75.69%\n",
      "Epoch [6/50] | Train Loss: 0.4424 | Train Acc: 78.06%\n",
      "Epoch [7/50] | Train Loss: 0.4189 | Train Acc: 79.02%\n",
      "Epoch [8/50] | Train Loss: 0.3461 | Train Acc: 83.99%\n",
      "Epoch [9/50] | Train Loss: 0.3210 | Train Acc: 84.95%\n",
      "Epoch [10/50] | Train Loss: 0.2929 | Train Acc: 87.55%\n",
      "Epoch [11/50] | Train Loss: 0.2470 | Train Acc: 89.55%\n",
      "Epoch [12/50] | Train Loss: 0.2466 | Train Acc: 89.92%\n",
      "Epoch [13/50] | Train Loss: 0.2200 | Train Acc: 91.25%\n",
      "Epoch [14/50] | Train Loss: 0.1869 | Train Acc: 91.18%\n",
      "Epoch [15/50] | Train Loss: 0.1693 | Train Acc: 93.11%\n",
      "Epoch [16/50] | Train Loss: 0.1492 | Train Acc: 93.55%\n",
      "Epoch [17/50] | Train Loss: 0.1513 | Train Acc: 93.40%\n",
      "Epoch [18/50] | Train Loss: 0.1145 | Train Acc: 95.70%\n",
      "Epoch [19/50] | Train Loss: 0.1155 | Train Acc: 95.26%\n",
      "Epoch [20/50] | Train Loss: 0.0805 | Train Acc: 97.26%\n",
      "Epoch [21/50] | Train Loss: 0.0747 | Train Acc: 97.18%\n",
      "Epoch [22/50] | Train Loss: 0.0986 | Train Acc: 96.89%\n",
      "Epoch [23/50] | Train Loss: 0.0718 | Train Acc: 97.18%\n",
      "Epoch [24/50] | Train Loss: 0.0501 | Train Acc: 98.52%\n",
      "Epoch [25/50] | Train Loss: 0.0982 | Train Acc: 96.66%\n",
      "Epoch [26/50] | Train Loss: 0.0467 | Train Acc: 98.37%\n",
      "Epoch [27/50] | Train Loss: 0.0342 | Train Acc: 98.89%\n",
      "Epoch [28/50] | Train Loss: 0.0508 | Train Acc: 98.07%\n",
      "Epoch [29/50] | Train Loss: 0.0363 | Train Acc: 98.81%\n",
      "Epoch [30/50] | Train Loss: 0.0154 | Train Acc: 99.33%\n",
      "Epoch [31/50] | Train Loss: 0.0588 | Train Acc: 98.07%\n",
      "Epoch [32/50] | Train Loss: 0.0479 | Train Acc: 98.00%\n",
      "Epoch [33/50] | Train Loss: 0.0322 | Train Acc: 99.11%\n",
      "Epoch [34/50] | Train Loss: 0.0214 | Train Acc: 99.26%\n",
      "Epoch [35/50] | Train Loss: 0.0099 | Train Acc: 99.63%\n",
      "Epoch [36/50] | Train Loss: 0.0026 | Train Acc: 100.00%\n",
      "Epoch [37/50] | Train Loss: 0.0023 | Train Acc: 100.00%\n",
      "Epoch [38/50] | Train Loss: 0.0579 | Train Acc: 98.44%\n",
      "Epoch [39/50] | Train Loss: 0.0344 | Train Acc: 99.18%\n",
      "Epoch [40/50] | Train Loss: 0.0221 | Train Acc: 99.41%\n",
      "Epoch [41/50] | Train Loss: 0.0281 | Train Acc: 99.26%\n",
      "Epoch [42/50] | Train Loss: 0.0061 | Train Acc: 99.93%\n",
      "Epoch [43/50] | Train Loss: 0.0041 | Train Acc: 99.93%\n",
      "Epoch [44/50] | Train Loss: 0.0151 | Train Acc: 99.78%\n",
      "Epoch [45/50] | Train Loss: 0.0011 | Train Acc: 100.00%\n",
      "Epoch [46/50] | Train Loss: 0.0009 | Train Acc: 100.00%\n",
      "Epoch [47/50] | Train Loss: 0.0006 | Train Acc: 100.00%\n",
      "Epoch [48/50] | Train Loss: 0.0004 | Train Acc: 100.00%\n",
      "Epoch [49/50] | Train Loss: 0.0004 | Train Acc: 100.00%\n",
      "Epoch [50/50] | Train Loss: 0.0003 | Train Acc: 100.00%\n",
      "Fold 3 Results -> Acc: 0.933, Precision: 0.931, Recall: 0.936, F1: 0.933\n",
      "\n",
      "========== Fold 4/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6839 | Train Acc: 56.63%\n",
      "Epoch [2/50] | Train Loss: 0.6756 | Train Acc: 57.67%\n",
      "Epoch [3/50] | Train Loss: 0.6312 | Train Acc: 66.49%\n",
      "Epoch [4/50] | Train Loss: 0.5950 | Train Acc: 70.64%\n",
      "Epoch [5/50] | Train Loss: 0.5786 | Train Acc: 70.05%\n",
      "Epoch [6/50] | Train Loss: 0.5815 | Train Acc: 70.79%\n",
      "Epoch [7/50] | Train Loss: 0.5679 | Train Acc: 72.13%\n",
      "Epoch [8/50] | Train Loss: 0.5579 | Train Acc: 71.53%\n",
      "Epoch [9/50] | Train Loss: 0.5501 | Train Acc: 72.28%\n",
      "Epoch [10/50] | Train Loss: 0.5280 | Train Acc: 73.39%\n",
      "Epoch [11/50] | Train Loss: 0.4932 | Train Acc: 75.61%\n",
      "Epoch [12/50] | Train Loss: 0.4502 | Train Acc: 79.02%\n",
      "Epoch [13/50] | Train Loss: 0.4180 | Train Acc: 80.28%\n",
      "Epoch [14/50] | Train Loss: 0.3924 | Train Acc: 82.80%\n",
      "Epoch [15/50] | Train Loss: 0.4561 | Train Acc: 78.50%\n",
      "Epoch [16/50] | Train Loss: 0.3651 | Train Acc: 83.47%\n",
      "Epoch [17/50] | Train Loss: 0.3269 | Train Acc: 85.62%\n",
      "Epoch [18/50] | Train Loss: 0.3013 | Train Acc: 86.95%\n",
      "Epoch [19/50] | Train Loss: 0.2923 | Train Acc: 88.14%\n",
      "Epoch [20/50] | Train Loss: 0.2673 | Train Acc: 88.88%\n",
      "Epoch [21/50] | Train Loss: 0.2828 | Train Acc: 88.29%\n",
      "Epoch [22/50] | Train Loss: 0.2501 | Train Acc: 90.21%\n",
      "Epoch [23/50] | Train Loss: 0.2285 | Train Acc: 91.18%\n",
      "Epoch [24/50] | Train Loss: 0.2222 | Train Acc: 90.88%\n",
      "Epoch [25/50] | Train Loss: 0.2057 | Train Acc: 92.29%\n",
      "Epoch [26/50] | Train Loss: 0.1968 | Train Acc: 92.66%\n",
      "Epoch [27/50] | Train Loss: 0.1969 | Train Acc: 92.81%\n",
      "Epoch [28/50] | Train Loss: 0.1883 | Train Acc: 93.11%\n",
      "Epoch [29/50] | Train Loss: 0.1802 | Train Acc: 92.74%\n",
      "Epoch [30/50] | Train Loss: 0.1777 | Train Acc: 93.62%\n",
      "Epoch [31/50] | Train Loss: 0.1697 | Train Acc: 93.40%\n",
      "Epoch [32/50] | Train Loss: 0.1635 | Train Acc: 94.00%\n",
      "Epoch [33/50] | Train Loss: 0.1450 | Train Acc: 94.59%\n",
      "Epoch [34/50] | Train Loss: 0.1369 | Train Acc: 95.03%\n",
      "Epoch [35/50] | Train Loss: 0.1422 | Train Acc: 94.44%\n",
      "Epoch [36/50] | Train Loss: 0.1577 | Train Acc: 94.00%\n",
      "Epoch [37/50] | Train Loss: 0.1222 | Train Acc: 95.26%\n",
      "Epoch [38/50] | Train Loss: 0.1437 | Train Acc: 94.66%\n",
      "Epoch [39/50] | Train Loss: 0.1511 | Train Acc: 94.44%\n",
      "Epoch [40/50] | Train Loss: 0.1080 | Train Acc: 96.00%\n",
      "Epoch [41/50] | Train Loss: 0.1077 | Train Acc: 96.07%\n",
      "Epoch [42/50] | Train Loss: 0.1253 | Train Acc: 94.89%\n",
      "Epoch [43/50] | Train Loss: 0.1008 | Train Acc: 96.52%\n",
      "Epoch [44/50] | Train Loss: 0.0894 | Train Acc: 96.89%\n",
      "Epoch [45/50] | Train Loss: 0.0805 | Train Acc: 97.33%\n",
      "Epoch [46/50] | Train Loss: 0.0884 | Train Acc: 96.52%\n",
      "Epoch [47/50] | Train Loss: 0.0801 | Train Acc: 97.11%\n",
      "Epoch [48/50] | Train Loss: 0.0784 | Train Acc: 97.11%\n",
      "Epoch [49/50] | Train Loss: 0.0769 | Train Acc: 97.11%\n",
      "Epoch [50/50] | Train Loss: 0.0651 | Train Acc: 97.85%\n",
      "Fold 4 Results -> Acc: 0.887, Precision: 0.885, Recall: 0.884, F1: 0.884\n",
      "\n",
      "========== Fold 5/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6851 | Train Acc: 56.56%\n",
      "Epoch [2/50] | Train Loss: 0.6772 | Train Acc: 56.93%\n",
      "Epoch [3/50] | Train Loss: 0.6391 | Train Acc: 64.27%\n",
      "Epoch [4/50] | Train Loss: 0.6019 | Train Acc: 68.12%\n",
      "Epoch [5/50] | Train Loss: 0.5840 | Train Acc: 71.09%\n",
      "Epoch [6/50] | Train Loss: 0.5809 | Train Acc: 71.09%\n",
      "Epoch [7/50] | Train Loss: 0.5681 | Train Acc: 72.13%\n",
      "Epoch [8/50] | Train Loss: 0.5594 | Train Acc: 71.09%\n",
      "Epoch [9/50] | Train Loss: 0.5515 | Train Acc: 72.57%\n",
      "Epoch [10/50] | Train Loss: 0.5454 | Train Acc: 72.87%\n",
      "Epoch [11/50] | Train Loss: 0.5164 | Train Acc: 73.91%\n",
      "Epoch [12/50] | Train Loss: 0.4649 | Train Acc: 77.09%\n",
      "Epoch [13/50] | Train Loss: 0.4237 | Train Acc: 79.91%\n",
      "Epoch [14/50] | Train Loss: 0.3775 | Train Acc: 82.65%\n",
      "Epoch [15/50] | Train Loss: 0.3539 | Train Acc: 84.06%\n",
      "Epoch [16/50] | Train Loss: 0.3137 | Train Acc: 86.58%\n",
      "Epoch [17/50] | Train Loss: 0.2858 | Train Acc: 87.47%\n",
      "Epoch [18/50] | Train Loss: 0.2634 | Train Acc: 88.88%\n",
      "Epoch [19/50] | Train Loss: 0.2273 | Train Acc: 90.36%\n",
      "Epoch [20/50] | Train Loss: 0.2101 | Train Acc: 91.70%\n",
      "Epoch [21/50] | Train Loss: 0.1844 | Train Acc: 92.74%\n",
      "Epoch [22/50] | Train Loss: 0.1635 | Train Acc: 94.29%\n",
      "Epoch [23/50] | Train Loss: 0.1257 | Train Acc: 95.55%\n",
      "Epoch [24/50] | Train Loss: 0.1033 | Train Acc: 96.37%\n",
      "Epoch [25/50] | Train Loss: 0.1087 | Train Acc: 95.85%\n",
      "Epoch [26/50] | Train Loss: 0.0745 | Train Acc: 97.26%\n",
      "Epoch [27/50] | Train Loss: 0.0671 | Train Acc: 97.85%\n",
      "Epoch [28/50] | Train Loss: 0.0474 | Train Acc: 98.44%\n",
      "Epoch [29/50] | Train Loss: 0.0696 | Train Acc: 97.55%\n",
      "Epoch [30/50] | Train Loss: 0.0364 | Train Acc: 99.18%\n",
      "Epoch [31/50] | Train Loss: 0.0482 | Train Acc: 98.37%\n",
      "Epoch [32/50] | Train Loss: 0.0416 | Train Acc: 98.59%\n",
      "Epoch [33/50] | Train Loss: 0.0148 | Train Acc: 99.70%\n",
      "Epoch [34/50] | Train Loss: 0.0248 | Train Acc: 99.41%\n",
      "Epoch [35/50] | Train Loss: 0.0087 | Train Acc: 99.85%\n",
      "Epoch [36/50] | Train Loss: 0.0443 | Train Acc: 98.81%\n",
      "Epoch [37/50] | Train Loss: 0.0383 | Train Acc: 98.67%\n",
      "Epoch [38/50] | Train Loss: 0.0286 | Train Acc: 99.33%\n",
      "Epoch [39/50] | Train Loss: 0.0081 | Train Acc: 99.85%\n",
      "Epoch [40/50] | Train Loss: 0.0181 | Train Acc: 99.33%\n",
      "Epoch [41/50] | Train Loss: 0.0536 | Train Acc: 98.44%\n",
      "Epoch [42/50] | Train Loss: 0.0075 | Train Acc: 99.78%\n",
      "Epoch [43/50] | Train Loss: 0.0019 | Train Acc: 100.00%\n",
      "Epoch [44/50] | Train Loss: 0.0007 | Train Acc: 100.00%\n",
      "Epoch [45/50] | Train Loss: 0.0004 | Train Acc: 100.00%\n",
      "Epoch [46/50] | Train Loss: 0.0003 | Train Acc: 100.00%\n",
      "Epoch [47/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [48/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [49/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [50/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Fold 5 Results -> Acc: 0.853, Precision: 0.851, Recall: 0.856, F1: 0.852\n",
      "\n",
      "========== Fold 6/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6853 | Train Acc: 55.30%\n",
      "Epoch [2/50] | Train Loss: 0.6697 | Train Acc: 58.71%\n",
      "Epoch [3/50] | Train Loss: 0.6402 | Train Acc: 63.38%\n",
      "Epoch [4/50] | Train Loss: 0.5954 | Train Acc: 69.90%\n",
      "Epoch [5/50] | Train Loss: 0.5835 | Train Acc: 71.39%\n",
      "Epoch [6/50] | Train Loss: 0.5537 | Train Acc: 71.09%\n",
      "Epoch [7/50] | Train Loss: 0.5212 | Train Acc: 73.83%\n",
      "Epoch [8/50] | Train Loss: 0.4910 | Train Acc: 74.94%\n",
      "Epoch [9/50] | Train Loss: 0.4429 | Train Acc: 79.39%\n",
      "Epoch [10/50] | Train Loss: 0.4017 | Train Acc: 81.76%\n",
      "Epoch [11/50] | Train Loss: 0.3547 | Train Acc: 83.32%\n",
      "Epoch [12/50] | Train Loss: 0.3375 | Train Acc: 85.77%\n",
      "Epoch [13/50] | Train Loss: 0.2686 | Train Acc: 88.44%\n",
      "Epoch [14/50] | Train Loss: 0.2309 | Train Acc: 91.70%\n",
      "Epoch [15/50] | Train Loss: 0.2032 | Train Acc: 92.44%\n",
      "Epoch [16/50] | Train Loss: 0.1662 | Train Acc: 94.14%\n",
      "Epoch [17/50] | Train Loss: 0.1580 | Train Acc: 93.92%\n",
      "Epoch [18/50] | Train Loss: 0.1299 | Train Acc: 95.40%\n",
      "Epoch [19/50] | Train Loss: 0.1098 | Train Acc: 96.07%\n",
      "Epoch [20/50] | Train Loss: 0.0964 | Train Acc: 96.59%\n",
      "Epoch [21/50] | Train Loss: 0.0808 | Train Acc: 97.63%\n",
      "Epoch [22/50] | Train Loss: 0.0669 | Train Acc: 97.70%\n",
      "Epoch [23/50] | Train Loss: 0.0480 | Train Acc: 98.67%\n",
      "Epoch [24/50] | Train Loss: 0.0499 | Train Acc: 98.44%\n",
      "Epoch [25/50] | Train Loss: 0.0530 | Train Acc: 98.07%\n",
      "Epoch [26/50] | Train Loss: 0.0325 | Train Acc: 98.67%\n",
      "Epoch [27/50] | Train Loss: 0.0134 | Train Acc: 99.63%\n",
      "Epoch [28/50] | Train Loss: 0.0171 | Train Acc: 99.48%\n",
      "Epoch [29/50] | Train Loss: 0.0557 | Train Acc: 98.30%\n",
      "Epoch [30/50] | Train Loss: 0.0359 | Train Acc: 98.96%\n",
      "Epoch [31/50] | Train Loss: 0.0196 | Train Acc: 99.48%\n",
      "Epoch [32/50] | Train Loss: 0.0040 | Train Acc: 100.00%\n",
      "Epoch [33/50] | Train Loss: 0.0084 | Train Acc: 99.70%\n",
      "Epoch [34/50] | Train Loss: 0.0134 | Train Acc: 99.70%\n",
      "Epoch [35/50] | Train Loss: 0.0484 | Train Acc: 98.30%\n",
      "Epoch [36/50] | Train Loss: 0.0095 | Train Acc: 99.78%\n",
      "Epoch [37/50] | Train Loss: 0.0039 | Train Acc: 99.85%\n",
      "Epoch [38/50] | Train Loss: 0.0012 | Train Acc: 100.00%\n",
      "Epoch [39/50] | Train Loss: 0.0006 | Train Acc: 100.00%\n",
      "Epoch [40/50] | Train Loss: 0.0005 | Train Acc: 100.00%\n",
      "Epoch [41/50] | Train Loss: 0.0003 | Train Acc: 100.00%\n",
      "Epoch [42/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [43/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [44/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [45/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [46/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [47/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [48/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [49/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [50/50] | Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Fold 6 Results -> Acc: 0.860, Precision: 0.864, Recall: 0.851, F1: 0.855\n",
      "\n",
      "========== Fold 7/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6841 | Train Acc: 56.63%\n",
      "Epoch [2/50] | Train Loss: 0.6534 | Train Acc: 58.71%\n",
      "Epoch [3/50] | Train Loss: 0.6091 | Train Acc: 67.31%\n",
      "Epoch [4/50] | Train Loss: 0.5523 | Train Acc: 71.98%\n",
      "Epoch [5/50] | Train Loss: 0.5103 | Train Acc: 73.91%\n",
      "Epoch [6/50] | Train Loss: 0.4506 | Train Acc: 77.32%\n",
      "Epoch [7/50] | Train Loss: 0.4578 | Train Acc: 78.87%\n",
      "Epoch [8/50] | Train Loss: 0.3980 | Train Acc: 81.47%\n",
      "Epoch [9/50] | Train Loss: 0.3669 | Train Acc: 84.06%\n",
      "Epoch [10/50] | Train Loss: 0.3274 | Train Acc: 85.77%\n",
      "Epoch [11/50] | Train Loss: 0.3040 | Train Acc: 86.58%\n",
      "Epoch [12/50] | Train Loss: 0.2708 | Train Acc: 88.44%\n",
      "Epoch [13/50] | Train Loss: 0.2646 | Train Acc: 88.36%\n",
      "Epoch [14/50] | Train Loss: 0.2106 | Train Acc: 91.62%\n",
      "Epoch [15/50] | Train Loss: 0.2166 | Train Acc: 91.03%\n",
      "Epoch [16/50] | Train Loss: 0.1960 | Train Acc: 91.85%\n",
      "Epoch [17/50] | Train Loss: 0.1731 | Train Acc: 93.33%\n",
      "Epoch [18/50] | Train Loss: 0.1675 | Train Acc: 94.00%\n",
      "Epoch [19/50] | Train Loss: 0.1412 | Train Acc: 94.59%\n",
      "Epoch [20/50] | Train Loss: 0.1315 | Train Acc: 95.48%\n",
      "Epoch [21/50] | Train Loss: 0.1400 | Train Acc: 94.96%\n",
      "Epoch [22/50] | Train Loss: 0.1184 | Train Acc: 95.85%\n",
      "Epoch [23/50] | Train Loss: 0.1152 | Train Acc: 96.00%\n",
      "Epoch [24/50] | Train Loss: 0.0824 | Train Acc: 97.55%\n",
      "Epoch [25/50] | Train Loss: 0.1114 | Train Acc: 96.00%\n",
      "Epoch [26/50] | Train Loss: 0.1143 | Train Acc: 96.37%\n",
      "Epoch [27/50] | Train Loss: 0.0826 | Train Acc: 97.41%\n",
      "Epoch [28/50] | Train Loss: 0.1012 | Train Acc: 96.59%\n",
      "Epoch [29/50] | Train Loss: 0.1126 | Train Acc: 96.37%\n",
      "Epoch [30/50] | Train Loss: 0.0548 | Train Acc: 97.85%\n",
      "Epoch [31/50] | Train Loss: 0.0519 | Train Acc: 98.30%\n",
      "Epoch [32/50] | Train Loss: 0.0486 | Train Acc: 98.52%\n",
      "Epoch [33/50] | Train Loss: 0.0540 | Train Acc: 98.37%\n",
      "Epoch [34/50] | Train Loss: 0.0556 | Train Acc: 98.30%\n",
      "Epoch [35/50] | Train Loss: 0.0294 | Train Acc: 99.18%\n",
      "Epoch [36/50] | Train Loss: 0.0275 | Train Acc: 99.04%\n",
      "Epoch [37/50] | Train Loss: 0.0216 | Train Acc: 99.26%\n",
      "Epoch [38/50] | Train Loss: 0.0627 | Train Acc: 98.30%\n",
      "Epoch [39/50] | Train Loss: 0.0450 | Train Acc: 99.04%\n",
      "Epoch [40/50] | Train Loss: 0.0709 | Train Acc: 98.15%\n",
      "Epoch [41/50] | Train Loss: 0.0243 | Train Acc: 99.33%\n",
      "Epoch [42/50] | Train Loss: 0.0110 | Train Acc: 99.78%\n",
      "Epoch [43/50] | Train Loss: 0.0091 | Train Acc: 99.85%\n",
      "Epoch [44/50] | Train Loss: 0.0177 | Train Acc: 99.56%\n",
      "Epoch [45/50] | Train Loss: 0.1221 | Train Acc: 96.15%\n",
      "Epoch [46/50] | Train Loss: 0.0236 | Train Acc: 99.48%\n",
      "Epoch [47/50] | Train Loss: 0.0163 | Train Acc: 99.56%\n",
      "Epoch [48/50] | Train Loss: 0.0086 | Train Acc: 99.85%\n",
      "Epoch [49/50] | Train Loss: 0.0076 | Train Acc: 99.93%\n",
      "Epoch [50/50] | Train Loss: 0.0070 | Train Acc: 99.93%\n",
      "Fold 7 Results -> Acc: 0.860, Precision: 0.857, Recall: 0.860, F1: 0.858\n",
      "\n",
      "========== Fold 8/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6830 | Train Acc: 56.63%\n",
      "Epoch [2/50] | Train Loss: 0.6677 | Train Acc: 55.37%\n",
      "Epoch [3/50] | Train Loss: 0.6599 | Train Acc: 60.12%\n",
      "Epoch [4/50] | Train Loss: 0.6082 | Train Acc: 66.64%\n",
      "Epoch [5/50] | Train Loss: 0.6017 | Train Acc: 69.98%\n",
      "Epoch [6/50] | Train Loss: 0.5772 | Train Acc: 71.91%\n",
      "Epoch [7/50] | Train Loss: 0.5417 | Train Acc: 73.46%\n",
      "Epoch [8/50] | Train Loss: 0.5042 | Train Acc: 75.69%\n",
      "Epoch [9/50] | Train Loss: 0.4738 | Train Acc: 77.69%\n",
      "Epoch [10/50] | Train Loss: 0.4413 | Train Acc: 79.39%\n",
      "Epoch [11/50] | Train Loss: 0.4039 | Train Acc: 81.47%\n",
      "Epoch [12/50] | Train Loss: 0.3663 | Train Acc: 83.69%\n",
      "Epoch [13/50] | Train Loss: 0.3298 | Train Acc: 85.25%\n",
      "Epoch [14/50] | Train Loss: 0.3033 | Train Acc: 86.81%\n",
      "Epoch [15/50] | Train Loss: 0.2683 | Train Acc: 88.66%\n",
      "Epoch [16/50] | Train Loss: 0.2253 | Train Acc: 90.07%\n",
      "Epoch [17/50] | Train Loss: 0.2148 | Train Acc: 91.62%\n",
      "Epoch [18/50] | Train Loss: 0.2085 | Train Acc: 91.77%\n",
      "Epoch [19/50] | Train Loss: 0.1575 | Train Acc: 94.51%\n",
      "Epoch [20/50] | Train Loss: 0.1590 | Train Acc: 94.14%\n",
      "Epoch [21/50] | Train Loss: 0.1372 | Train Acc: 94.81%\n",
      "Epoch [22/50] | Train Loss: 0.1089 | Train Acc: 95.85%\n",
      "Epoch [23/50] | Train Loss: 0.1094 | Train Acc: 96.37%\n",
      "Epoch [24/50] | Train Loss: 0.1001 | Train Acc: 96.74%\n",
      "Epoch [25/50] | Train Loss: 0.0845 | Train Acc: 97.26%\n",
      "Epoch [26/50] | Train Loss: 0.0557 | Train Acc: 98.22%\n",
      "Epoch [27/50] | Train Loss: 0.0481 | Train Acc: 98.22%\n",
      "Epoch [28/50] | Train Loss: 0.0552 | Train Acc: 97.70%\n",
      "Epoch [29/50] | Train Loss: 0.0718 | Train Acc: 97.48%\n",
      "Epoch [30/50] | Train Loss: 0.0334 | Train Acc: 99.04%\n",
      "Epoch [31/50] | Train Loss: 0.0229 | Train Acc: 99.48%\n",
      "Epoch [32/50] | Train Loss: 0.0197 | Train Acc: 99.56%\n",
      "Epoch [33/50] | Train Loss: 0.0316 | Train Acc: 99.18%\n",
      "Epoch [34/50] | Train Loss: 0.0337 | Train Acc: 98.52%\n",
      "Epoch [35/50] | Train Loss: 0.1117 | Train Acc: 97.11%\n",
      "Epoch [36/50] | Train Loss: 0.0257 | Train Acc: 99.26%\n",
      "Epoch [37/50] | Train Loss: 0.0093 | Train Acc: 99.78%\n",
      "Epoch [38/50] | Train Loss: 0.0046 | Train Acc: 100.00%\n",
      "Epoch [39/50] | Train Loss: 0.0018 | Train Acc: 100.00%\n",
      "Epoch [40/50] | Train Loss: 0.0011 | Train Acc: 100.00%\n",
      "Epoch [41/50] | Train Loss: 0.0008 | Train Acc: 100.00%\n",
      "Epoch [42/50] | Train Loss: 0.0006 | Train Acc: 100.00%\n",
      "Epoch [43/50] | Train Loss: 0.0230 | Train Acc: 99.11%\n",
      "Epoch [44/50] | Train Loss: 0.0428 | Train Acc: 98.74%\n",
      "Epoch [45/50] | Train Loss: 0.0441 | Train Acc: 98.59%\n",
      "Epoch [46/50] | Train Loss: 0.0045 | Train Acc: 100.00%\n",
      "Epoch [47/50] | Train Loss: 0.0302 | Train Acc: 99.04%\n",
      "Epoch [48/50] | Train Loss: 0.0174 | Train Acc: 99.63%\n",
      "Epoch [49/50] | Train Loss: 0.0070 | Train Acc: 99.78%\n",
      "Epoch [50/50] | Train Loss: 0.0018 | Train Acc: 100.00%\n",
      "Fold 8 Results -> Acc: 0.867, Precision: 0.871, Recall: 0.877, F1: 0.866\n",
      "\n",
      "========== Fold 9/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6855 | Train Acc: 56.26%\n",
      "Epoch [2/50] | Train Loss: 0.6694 | Train Acc: 58.34%\n",
      "Epoch [3/50] | Train Loss: 0.6378 | Train Acc: 65.46%\n",
      "Epoch [4/50] | Train Loss: 0.5998 | Train Acc: 69.83%\n",
      "Epoch [5/50] | Train Loss: 0.5712 | Train Acc: 70.79%\n",
      "Epoch [6/50] | Train Loss: 0.5445 | Train Acc: 74.87%\n",
      "Epoch [7/50] | Train Loss: 0.5281 | Train Acc: 74.57%\n",
      "Epoch [8/50] | Train Loss: 0.5056 | Train Acc: 75.32%\n",
      "Epoch [9/50] | Train Loss: 0.4714 | Train Acc: 77.09%\n",
      "Epoch [10/50] | Train Loss: 0.4544 | Train Acc: 78.21%\n",
      "Epoch [11/50] | Train Loss: 0.4051 | Train Acc: 81.39%\n",
      "Epoch [12/50] | Train Loss: 0.3779 | Train Acc: 82.43%\n",
      "Epoch [13/50] | Train Loss: 0.3509 | Train Acc: 83.99%\n",
      "Epoch [14/50] | Train Loss: 0.3170 | Train Acc: 84.66%\n",
      "Epoch [15/50] | Train Loss: 0.2826 | Train Acc: 86.88%\n",
      "Epoch [16/50] | Train Loss: 0.2673 | Train Acc: 88.44%\n",
      "Epoch [17/50] | Train Loss: 0.2272 | Train Acc: 91.48%\n",
      "Epoch [18/50] | Train Loss: 0.2011 | Train Acc: 91.55%\n",
      "Epoch [19/50] | Train Loss: 0.1947 | Train Acc: 92.14%\n",
      "Epoch [20/50] | Train Loss: 0.1685 | Train Acc: 93.25%\n",
      "Epoch [21/50] | Train Loss: 0.1530 | Train Acc: 93.85%\n",
      "Epoch [22/50] | Train Loss: 0.1083 | Train Acc: 95.26%\n",
      "Epoch [23/50] | Train Loss: 0.0856 | Train Acc: 96.81%\n",
      "Epoch [24/50] | Train Loss: 0.0945 | Train Acc: 96.59%\n",
      "Epoch [25/50] | Train Loss: 0.0764 | Train Acc: 96.74%\n",
      "Epoch [26/50] | Train Loss: 0.0467 | Train Acc: 98.44%\n",
      "Epoch [27/50] | Train Loss: 0.0519 | Train Acc: 98.07%\n",
      "Epoch [28/50] | Train Loss: 0.0356 | Train Acc: 98.74%\n",
      "Epoch [29/50] | Train Loss: 0.0181 | Train Acc: 99.41%\n",
      "Epoch [30/50] | Train Loss: 0.0523 | Train Acc: 98.30%\n",
      "Epoch [31/50] | Train Loss: 0.0290 | Train Acc: 99.11%\n",
      "Epoch [32/50] | Train Loss: 0.0547 | Train Acc: 98.37%\n",
      "Epoch [33/50] | Train Loss: 0.0203 | Train Acc: 99.56%\n",
      "Epoch [34/50] | Train Loss: 0.0109 | Train Acc: 99.85%\n",
      "Epoch [35/50] | Train Loss: 0.0096 | Train Acc: 99.63%\n",
      "Epoch [36/50] | Train Loss: 0.0031 | Train Acc: 99.93%\n",
      "Epoch [37/50] | Train Loss: 0.0672 | Train Acc: 97.70%\n",
      "Epoch [38/50] | Train Loss: 0.0156 | Train Acc: 99.48%\n",
      "Epoch [39/50] | Train Loss: 0.0060 | Train Acc: 99.78%\n",
      "Epoch [40/50] | Train Loss: 0.0056 | Train Acc: 99.70%\n",
      "Epoch [41/50] | Train Loss: 0.0012 | Train Acc: 100.00%\n",
      "Epoch [42/50] | Train Loss: 0.0005 | Train Acc: 100.00%\n",
      "Epoch [43/50] | Train Loss: 0.0003 | Train Acc: 100.00%\n",
      "Epoch [44/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [45/50] | Train Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch [46/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [47/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [48/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [49/50] | Train Loss: 0.0001 | Train Acc: 100.00%\n",
      "Epoch [50/50] | Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Fold 9 Results -> Acc: 0.840, Precision: 0.837, Recall: 0.837, F1: 0.837\n",
      "\n",
      "========== Fold 10/10 ==========\n",
      "Detected input size: 6\n",
      "Epoch [1/50] | Train Loss: 0.6969 | Train Acc: 50.96%\n",
      "Epoch [2/50] | Train Loss: 0.6794 | Train Acc: 56.67%\n",
      "Epoch [3/50] | Train Loss: 0.6738 | Train Acc: 56.52%\n",
      "Epoch [4/50] | Train Loss: 0.6625 | Train Acc: 60.52%\n",
      "Epoch [5/50] | Train Loss: 0.6357 | Train Acc: 65.70%\n",
      "Epoch [6/50] | Train Loss: 0.6166 | Train Acc: 69.26%\n",
      "Epoch [7/50] | Train Loss: 0.5922 | Train Acc: 70.15%\n",
      "Epoch [8/50] | Train Loss: 0.5866 | Train Acc: 71.56%\n",
      "Epoch [9/50] | Train Loss: 0.5664 | Train Acc: 72.07%\n",
      "Epoch [10/50] | Train Loss: 0.5655 | Train Acc: 72.59%\n",
      "Epoch [11/50] | Train Loss: 0.5581 | Train Acc: 73.19%\n",
      "Epoch [12/50] | Train Loss: 0.5534 | Train Acc: 72.59%\n",
      "Epoch [13/50] | Train Loss: 0.5516 | Train Acc: 73.33%\n",
      "Epoch [14/50] | Train Loss: 0.5480 | Train Acc: 73.26%\n",
      "Epoch [15/50] | Train Loss: 0.5423 | Train Acc: 73.11%\n",
      "Epoch [16/50] | Train Loss: 0.5391 | Train Acc: 73.04%\n",
      "Epoch [17/50] | Train Loss: 0.5361 | Train Acc: 74.44%\n",
      "Epoch [18/50] | Train Loss: 0.5315 | Train Acc: 74.30%\n",
      "Epoch [19/50] | Train Loss: 0.5249 | Train Acc: 74.07%\n",
      "Epoch [20/50] | Train Loss: 0.5224 | Train Acc: 74.00%\n",
      "Epoch [21/50] | Train Loss: 0.5119 | Train Acc: 75.33%\n",
      "Epoch [22/50] | Train Loss: 0.5020 | Train Acc: 76.15%\n",
      "Epoch [23/50] | Train Loss: 0.5051 | Train Acc: 74.96%\n",
      "Epoch [24/50] | Train Loss: 0.4893 | Train Acc: 75.56%\n",
      "Epoch [25/50] | Train Loss: 0.4934 | Train Acc: 76.30%\n",
      "Epoch [26/50] | Train Loss: 0.4971 | Train Acc: 76.81%\n",
      "Epoch [27/50] | Train Loss: 0.4763 | Train Acc: 75.78%\n",
      "Epoch [28/50] | Train Loss: 0.4746 | Train Acc: 77.11%\n",
      "Epoch [29/50] | Train Loss: 0.4635 | Train Acc: 75.70%\n",
      "Epoch [30/50] | Train Loss: 0.4623 | Train Acc: 77.26%\n",
      "Epoch [31/50] | Train Loss: 0.4848 | Train Acc: 76.52%\n",
      "Epoch [32/50] | Train Loss: 0.4520 | Train Acc: 78.07%\n",
      "Epoch [33/50] | Train Loss: 0.4513 | Train Acc: 78.37%\n",
      "Epoch [34/50] | Train Loss: 0.4527 | Train Acc: 78.89%\n",
      "Epoch [35/50] | Train Loss: 0.4455 | Train Acc: 77.56%\n",
      "Epoch [36/50] | Train Loss: 0.4569 | Train Acc: 77.56%\n",
      "Epoch [37/50] | Train Loss: 0.4413 | Train Acc: 79.78%\n",
      "Epoch [38/50] | Train Loss: 0.4345 | Train Acc: 79.63%\n",
      "Epoch [39/50] | Train Loss: 0.4375 | Train Acc: 79.11%\n",
      "Epoch [40/50] | Train Loss: 0.4337 | Train Acc: 79.48%\n",
      "Epoch [41/50] | Train Loss: 0.4198 | Train Acc: 80.67%\n",
      "Epoch [42/50] | Train Loss: 0.4255 | Train Acc: 80.81%\n",
      "Epoch [43/50] | Train Loss: 0.4245 | Train Acc: 80.96%\n",
      "Epoch [44/50] | Train Loss: 0.4134 | Train Acc: 81.04%\n",
      "Epoch [45/50] | Train Loss: 0.4140 | Train Acc: 82.07%\n",
      "Epoch [46/50] | Train Loss: 0.4248 | Train Acc: 80.15%\n",
      "Epoch [47/50] | Train Loss: 0.3980 | Train Acc: 81.19%\n",
      "Epoch [48/50] | Train Loss: 0.4082 | Train Acc: 81.48%\n",
      "Epoch [49/50] | Train Loss: 0.4017 | Train Acc: 82.37%\n",
      "Epoch [50/50] | Train Loss: 0.4023 | Train Acc: 81.85%\n",
      "Fold 10 Results -> Acc: 0.785, Precision: 0.786, Recall: 0.775, F1: 0.778\n",
      "\n",
      "========== Cross-Validation Results ==========\n",
      "Accuracy:  0.870 ± 0.040\n",
      "Precision: 0.869 ± 0.039\n",
      "Recall:    0.869 ± 0.042\n",
      "F1-score:  0.867 ± 0.041\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ScanpathDataset(\"Scanpaths/all\", \"Raw Data/all\", transform=transform)\n",
    "print(\"Total samples:\", len(dataset))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Get all labels from dataset for stratification\n",
    "# ------------------------------------------------\n",
    "all_labels = [dataset[i][2] for i in range(len(dataset))]  # assuming (image, seq, label)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Stratified K-Fold setup\n",
    "# ------------------------------------------------\n",
    "num_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(all_labels)), all_labels)):\n",
    "    print(f\"\\n========== Fold {fold+1}/{num_folds} ==========\")\n",
    "    \n",
    "    # Create DataLoaders for this fold\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_subset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # infer input_size from one CSV\n",
    "    sample_img, sample_seq, _ = dataset[0]\n",
    "    input_size = sample_seq.shape[1]\n",
    "    print(\"Detected input size:\", input_size)\n",
    "\n",
    "    # Re-initialize the model for each fold\n",
    "    model = VTNet(input_size=input_size, rnn_type='gru').to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training\n",
    "    # -----------------------------\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, sequences, seq_lengths, labels in train_loader:\n",
    "            images, sequences, seq_lengths, labels = (\n",
    "                images.to(device),\n",
    "                sequences.to(device),\n",
    "                seq_lengths.to(device),\n",
    "                labels.to(device)\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, sequences, seq_lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # --- optional: print intermediate training progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Validation\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, sequences, seq_lengths, labels in val_loader:\n",
    "            images, sequences, seq_lengths, labels = (\n",
    "                images.to(device),\n",
    "                sequences.to(device),\n",
    "                seq_lengths.to(device),\n",
    "                labels.to(device)\n",
    "            )\n",
    "            outputs = model(images, sequences, seq_lengths)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics for this fold\n",
    "    acc = accuracy_score(all_true, all_preds)\n",
    "    precision = precision_score(all_true, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_true, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_true, all_preds, average=\"macro\")\n",
    "\n",
    "    fold_metrics.append((acc, precision, recall, f1))\n",
    "    print(f\"Fold {fold+1} Results -> Acc: {acc:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Average metrics across folds\n",
    "# ------------------------------------------------\n",
    "fold_metrics = np.array(fold_metrics)\n",
    "mean_metrics = fold_metrics.mean(axis=0)\n",
    "std_metrics = fold_metrics.std(axis=0)\n",
    "\n",
    "print(\"\\n========== Cross-Validation Results ==========\")\n",
    "print(f\"Accuracy:  {mean_metrics[0]:.3f} ± {std_metrics[0]:.3f}\")\n",
    "print(f\"Precision: {mean_metrics[1]:.3f} ± {std_metrics[1]:.3f}\")\n",
    "print(f\"Recall:    {mean_metrics[2]:.3f} ± {std_metrics[2]:.3f}\")\n",
    "print(f\"F1-score:  {mean_metrics[3]:.3f} ± {std_metrics[3]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac09ba1",
   "metadata": {},
   "source": [
    "## Model Evaluation: Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e22ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    literate       0.79      0.69      0.74        65\n",
      "  illiterate       0.78      0.86      0.82        84\n",
      "\n",
      "    accuracy                           0.79       149\n",
      "   macro avg       0.79      0.77      0.78       149\n",
      "weighted avg       0.79      0.79      0.78       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# --- Collect predictions and labels ---\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, sequences, seq_lengths, labels in val_loader:\n",
    "        images, sequences, seq_lengths, labels = (\n",
    "            images.to(device),\n",
    "            sequences.to(device),\n",
    "            seq_lengths.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "\n",
    "        outputs = model(images, sequences, seq_lengths)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "classes = [\"literate\", \"illiterate\"]  # matches label_map: literate=0, illiterate=1\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(classification_report(y_true, y_pred, target_names=classes, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df45b92",
   "metadata": {},
   "source": [
    "## Model Testing and Debugging\n",
    "\n",
    "The following cells test the model with dummy data to verify the architecture and output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c201822",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_len = 30     # number of timesteps in your series\n",
    "input_size = 6   # features per timestep (FPOGX, FPOGY, LPS, RPS, LPMM, RPMM)\n",
    "\n",
    "# generate dummy inputs\n",
    "scan_path = torch.randn(batch_size, 1, 150, 150, device=device, dtype=torch.float32)\n",
    "time_series = torch.randn(batch_size, seq_len, input_size, device=device, dtype=torch.float32)\n",
    "seq_lengths = torch.tensor([seq_len] * batch_size, dtype=torch.long, device=device)  # all full-length sequences\n",
    "\n",
    "# run forward pass (no hidden needed)\n",
    "output = model(scan_path, time_series, seq_lengths)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output logits:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
