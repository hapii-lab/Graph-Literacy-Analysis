{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d99dd1",
   "metadata": {},
   "source": [
    "# VTNet Leave-One-User-Out Evaluation with 3-Second DGM Features\n",
    "\n",
    "This notebook mirrors the leave-one-user-out (LOUO) evaluation performed on raw gaze coordinates, but replaces the temporal branch with Dynamic Gaze Metrics (DGMs) computed over 3-second tumbling windows. The CNN branch still consumes non-contaminated scanpath images while the RNN branch processes DGM sequences. The goal is to assess cross-user generalisation on the clean dataset for each graph type (`bar`, `line`, `pie`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fb4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"\\n=== GPU Diagnostics ===\")\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "try:\n",
    "    print(f\"CUDA available: {tf.test.is_gpu_available(cuda_only=True)}\")\n",
    "except Exception:\n",
    "    print(\"CUDA availability check failed (method may be deprecated)\")\n",
    "\n",
    "print(f\"\\nPhysical devices: {tf.config.list_physical_devices()}\")\n",
    "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Configure GPU memory growth if possible\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"\\n✅ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Warning: Could not configure GPU memory growth: {e}\")\n",
    "else:\n",
    "    print(\"\\n❌ No GPU detected by TensorFlow. Training will fall back to CPU.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ff031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(start: Path, marker: str = \"non-contaminated datasets\", max_levels: int = 6) -> Path:\n",
    "    current = start.resolve()\n",
    "    for _ in range(max_levels):\n",
    "        if (current / marker).exists():\n",
    "            return current\n",
    "        current = current.parent\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate project root containing '{marker}' starting from {start}\"\n",
    "    )\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "try:\n",
    "    base_dir = find_project_root(current_dir)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    base_dir = current_dir  # fallback to current directory\n",
    "\n",
    "scanpaths_dir = base_dir / \"non-contaminated datasets\" / \"Scanpaths\"\n",
    "dgm_dir = (\n",
    "    base_dir\n",
    "    / \"non-contaminated datasets\"\n",
    "    / \"Organized Normalized Tumbling Window DGMs (3s)\"\n",
    ")\n",
    "raw_csv_dir = base_dir / \"non-contaminated datasets\" / \"Raw Eye Tracking Data\"\n",
    "target_csv_path = base_dir / \"Code\" / \"Utilities\" / \"users_literacy_results.csv\"\n",
    "\n",
    "print(f\"Working directory: {current_dir}\")\n",
    "print(f\"Project root: {base_dir}\")\n",
    "print(f\"Scanpaths dir exists: {scanpaths_dir.exists()}\")\n",
    "print(f\"DGM dir exists: {dgm_dir.exists()}\")\n",
    "print(f\"Raw CSV dir exists: {raw_csv_dir.exists()}\")\n",
    "print(f\"Target CSV path: {target_csv_path}\")\n",
    "\n",
    "# Data constants\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "SEQ_LENGTH = 20  # maximum number of DGM windows\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 12\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "skip_users = {5, 20}\n",
    "graph_types = [\"bar\", \"line\", \"pie\"]\n",
    "class_map = {\"illiterate\": 0, \"literate\": 1}\n",
    "label_names = [\"illiterate\", \"literate\"]\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1175df",
   "metadata": {},
   "outputs": [],
   "source": [
    "literacy_df = pd.read_csv(target_csv_path)\n",
    "literacy_df[\"MEDIA_ID\"] = literacy_df[\"MEDIA_ID\"].astype(int)\n",
    "literacy_df[\"LITERACY\"] = literacy_df[\"LITERACY\"].astype(int)\n",
    "literacy_map = dict(zip(literacy_df[\"MEDIA_ID\"], literacy_df[\"LITERACY\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_user_id(path: Path) -> int | None:\n",
    "    name = path.name\n",
    "    if name.startswith(\"user_\"):\n",
    "        try:\n",
    "            return int(name.split(\"_\")[1])\n",
    "        except (IndexError, ValueError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_question_id(filename: str) -> int | None:\n",
    "    try:\n",
    "        part = filename.split(\"question_\")[1]\n",
    "        return int(part.split(\"_\")[0].split(\".\")[0])\n",
    "    except (IndexError, ValueError):\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def get_dgm_feature_indices() -> list[int]:\n",
    "    columns_to_exclude = {\n",
    "        \"sum_peak_saccade_velocity\",\n",
    "        \"mean_peak_saccade_velocity\",\n",
    "        \"median_peak_saccade_velocity\",\n",
    "        \"std_peak_saccade_velocity\",\n",
    "        \"min_peak_saccade_velocity\",\n",
    "        \"max_peak_saccade_velocity\",\n",
    "        \"sum_mean_saccade_velocity\",\n",
    "        \"mean_mean_saccade_velocity\",\n",
    "        \"median_mean_saccade_velocity\",\n",
    "        \"std_mean_saccade_velocity\",\n",
    "        \"min_mean_saccade_velocity\",\n",
    "        \"max_mean_saccade_velocity\",\n",
    "        \"stationary_entropy\",\n",
    "        \"transition_entropy\",\n",
    "        \"total_number_of_blinks\",\n",
    "        \"average_blink_rate_per_minute\",\n",
    "        \"total_number_of_l_mouse_clicks\",\n",
    "        \"average_pupil_size_of_left_eye\",\n",
    "        \"average_pupil_size_of_right_eye\",\n",
    "        \"average_pupil_size_of_both_eyes\",\n",
    "    }\n",
    "\n",
    "    all_columns = [\n",
    "        \"total_number_of_fixations\",\n",
    "        \"sum_of_all_fixation_duration_s\",\n",
    "        \"mean_fixation_duration_s\",\n",
    "        \"median_fixation_duration_s\",\n",
    "        \"stdev_of_fixation_durations_s\",\n",
    "        \"min_fixation_duration_s\",\n",
    "        \"max_fixation_duration_s\",\n",
    "        \"total_number_of_saccades\",\n",
    "        \"sum_of_all_saccade_lengths\",\n",
    "        \"mean_saccade_length\",\n",
    "        \"median_saccade_length\",\n",
    "        \"stdev_of_saccade_lengths\",\n",
    "        \"min_saccade_length\",\n",
    "        \"max_saccade_length\",\n",
    "        \"sum_of_all_saccade_durations\",\n",
    "        \"mean_saccade_duration\",\n",
    "        \"median_saccade_duration\",\n",
    "        \"stdev_of_saccade_durations\",\n",
    "        \"min_saccade_duration\",\n",
    "        \"max_saccade_duration\",\n",
    "        \"sum_of_all_saccade_amplitudes\",\n",
    "        \"mean_saccade_amplitude\",\n",
    "        \"median_saccade_amplitude\",\n",
    "        \"stdev_of_saccade_amplitude\",\n",
    "        \"min_saccade_amplitude\",\n",
    "        \"max_saccade_amplitude\",\n",
    "        \"scanpath_duration\",\n",
    "        \"fixation_to_saccade_ratio\",\n",
    "        \"sum_peak_saccade_velocity\",\n",
    "        \"mean_peak_saccade_velocity\",\n",
    "        \"median_peak_saccade_velocity\",\n",
    "        \"std_peak_saccade_velocity\",\n",
    "        \"min_peak_saccade_velocity\",\n",
    "        \"max_peak_saccade_velocity\",\n",
    "        \"sum_mean_saccade_velocity\",\n",
    "        \"mean_mean_saccade_velocity\",\n",
    "        \"median_mean_saccade_velocity\",\n",
    "        \"std_mean_saccade_velocity\",\n",
    "        \"min_mean_saccade_velocity\",\n",
    "        \"max_mean_saccade_velocity\",\n",
    "        \"sum_of_all_absolute_degrees\",\n",
    "        \"mean_absolute_degree\",\n",
    "        \"median_absolute_degree\",\n",
    "        \"stdev_of_absolute_degrees\",\n",
    "        \"min_absolute_degree\",\n",
    "        \"max_absolute_degree\",\n",
    "        \"sum_of_all_relative_degrees\",\n",
    "        \"mean_relative_degree\",\n",
    "        \"median_relative_degree\",\n",
    "        \"stdev_of_relative_degrees\",\n",
    "        \"min_relative_degree\",\n",
    "        \"max_relative_degree\",\n",
    "        \"convex_hull_area\",\n",
    "        \"stationary_entropy\",\n",
    "        \"transition_entropy\",\n",
    "        \"total_number_of_blinks\",\n",
    "        \"average_blink_rate_per_minute\",\n",
    "        \"total_number_of_valid_recordings\",\n",
    "        \"average_pupil_size_of_left_eye\",\n",
    "        \"average_pupil_size_of_right_eye\",\n",
    "        \"average_pupil_size_of_both_eyes\",\n",
    "        \"total_number_of_l_mouse_clicks\",\n",
    "        \"beginning_timestamp\",\n",
    "        \"ending_timestamp\",\n",
    "        \"window_duration\",\n",
    "        \"initial_seconds_elapsed_since_start\",\n",
    "        \"final_seconds_elapsed_since_start\",\n",
    "        \"delta_average_pupil_size_of_left_eye\",\n",
    "        \"delta_average_pupil_size_of_right_eye\",\n",
    "        \"delta_average_pupil_size_of_both_eyes\",\n",
    "    ]\n",
    "\n",
    "    return [idx for idx, col in enumerate(all_columns) if col not in columns_to_exclude]\n",
    "\n",
    "\n",
    "def load_dgm_from_csv(csv_path: Path, max_sequence_length: int = SEQ_LENGTH) -> np.ndarray:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        keep_indices = get_dgm_feature_indices()\n",
    "        if len(df.columns) < len(keep_indices):\n",
    "            raise ValueError(\n",
    "                f\"DGM file {csv_path} has insufficient columns: {len(df.columns)}\"\n",
    "            )\n",
    "        data = df.iloc[:, keep_indices].to_numpy(dtype=np.float32)\n",
    "        data = np.nan_to_num(data, nan=0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading DGM {csv_path}: {e}\")\n",
    "        data = np.zeros((0, len(get_dgm_feature_indices())), dtype=np.float32)\n",
    "\n",
    "    if data.shape[0] >= max_sequence_length:\n",
    "        data = data[-max_sequence_length:]\n",
    "    else:\n",
    "        pad_len = max_sequence_length - data.shape[0]\n",
    "        padding = np.zeros((pad_len, data.shape[1]), dtype=np.float32)\n",
    "        data = np.vstack([data, padding])\n",
    "\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_dgm_path_from_image_path(image_path: Path, base_dgm_dir: Path) -> Path:\n",
    "    filename = image_path.name.replace(\"_scanpath.png\", \"_tumbling_all_window_DGMs.csv\")\n",
    "    if \"illiterate\" in image_path.parts:\n",
    "        return base_dgm_dir / \"illiterate\" / filename\n",
    "    if \"literate\" in image_path.parts:\n",
    "        return base_dgm_dir / \"literate\" / filename\n",
    "    raise ValueError(f\"Could not infer literacy label from image path: {image_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47472811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples_for_graph(graph_type: str):\n",
    "    samples = []\n",
    "    scan_graph_dir = scanpaths_dir / graph_type\n",
    "    dgm_graph_dir = dgm_dir / graph_type\n",
    "\n",
    "    for literacy_name, label in class_map.items():\n",
    "        scan_class_dir = scan_graph_dir / literacy_name\n",
    "        dgm_class_dir = dgm_graph_dir / literacy_name\n",
    "        if not scan_class_dir.exists() or not dgm_class_dir.exists():\n",
    "            continue\n",
    "\n",
    "        for user_dir in sorted(scan_class_dir.glob(\"user_*\")):\n",
    "            user_id = parse_user_id(user_dir)\n",
    "            if user_id is None or user_id in skip_users:\n",
    "                continue\n",
    "\n",
    "            dgm_user_dir = dgm_class_dir / f\"user_{user_id}\"\n",
    "            if not dgm_user_dir.exists():\n",
    "                continue\n",
    "\n",
    "            for img_path in sorted(user_dir.glob(\"*.png\")):\n",
    "                question_id = parse_question_id(img_path.name)\n",
    "                dgm_filename = img_path.name.replace(\n",
    "                    \"_scanpath.png\", \"_tumbling_all_window_DGMs.csv\"\n",
    "                )\n",
    "                dgm_path = dgm_user_dir / dgm_filename\n",
    "                if not dgm_path.exists():\n",
    "                    continue\n",
    "\n",
    "                samples.append(\n",
    "                    {\n",
    "                        \"image\": img_path,\n",
    "                        \"dgm\": dgm_path,\n",
    "                        \"label\": label,\n",
    "                        \"user_id\": user_id,\n",
    "                        \"question_id\": question_id,\n",
    "                    }\n",
    "                )\n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _py_load_dgm(path_tensor):\n",
    "    if hasattr(path_tensor, \"numpy\"):\n",
    "        path_bytes = path_tensor.numpy()\n",
    "    else:\n",
    "        path_bytes = path_tensor\n",
    "    if isinstance(path_bytes, bytes):\n",
    "        path_str = path_bytes.decode(\"utf-8\")\n",
    "    else:\n",
    "        path_str = str(path_bytes)\n",
    "    return load_dgm_from_csv(Path(path_str))\n",
    "\n",
    "\n",
    "def make_dataset(samples, shuffle=False, batch_size=BATCH_SIZE):\n",
    "    if len(samples) == 0:\n",
    "        raise ValueError(\"No samples provided to build dataset.\")\n",
    "\n",
    "    image_paths = [str(s[\"image\"]) for s in samples]\n",
    "    dgm_paths = [str(s[\"dgm\"]) for s in samples]\n",
    "    labels = [s[\"label\"] for s in samples]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, dgm_paths, labels))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(samples), reshuffle_each_iteration=True)\n",
    "\n",
    "    num_features = len(get_dgm_feature_indices())\n",
    "\n",
    "    def load_sample(image_path, dgm_path, label):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "        image = tf.image.rgb_to_grayscale(image)\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "        dgm = tf.py_function(func=_py_load_dgm, inp=[dgm_path], Tout=tf.float32)\n",
    "        dgm.set_shape((SEQ_LENGTH, num_features))\n",
    "\n",
    "        label_onehot = tf.one_hot(label, depth=2)\n",
    "        return (image, dgm), label_onehot\n",
    "\n",
    "    dataset = dataset.map(load_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vtnet_dgm_model(rnn_type: str = \"gru\", hidden_size: int = 256):\n",
    "    scanpath_input = tf.keras.layers.Input(\n",
    "        shape=(IMG_HEIGHT, IMG_WIDTH, 1), name=\"scanpath_input\"\n",
    "    )\n",
    "    dgm_input = tf.keras.layers.Input(\n",
    "        shape=(SEQ_LENGTH, len(get_dgm_feature_indices())), name=\"dgm_input\"\n",
    "    )\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(6, (5, 5), activation=\"relu\", name=\"conv1\")(scanpath_input)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "    x = tf.keras.layers.Conv2D(16, (5, 5), activation=\"relu\", name=\"conv2\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "    x = tf.keras.layers.Flatten(name=\"cnn_flatten\")(x)\n",
    "    cnn_features = tf.keras.layers.Dense(50, activation=\"relu\", name=\"cnn_fc1\")(x)\n",
    "\n",
    "    if rnn_type.lower() == \"gru\":\n",
    "        rnn_layer = tf.keras.layers.GRU(hidden_size, name=\"dgm_gru\")\n",
    "    elif rnn_type.lower() == \"lstm\":\n",
    "        rnn_layer = tf.keras.layers.LSTM(hidden_size, name=\"dgm_lstm\")\n",
    "    else:\n",
    "        rnn_layer = tf.keras.layers.SimpleRNN(hidden_size, name=\"dgm_simple\")\n",
    "\n",
    "    dgm_features = rnn_layer(dgm_input)\n",
    "\n",
    "    fused = tf.keras.layers.Concatenate(name=\"fusion_concat\")(\n",
    "        [cnn_features, dgm_features]\n",
    "    )\n",
    "    fused = tf.keras.layers.Dense(20, activation=\"relu\", name=\"fusion_fc1\")(fused)\n",
    "    output = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"output\")(fused)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[scanpath_input, dgm_input], outputs=output, name=\"VTNet_LOUO_DGM\"\n",
    "    )\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(train_samples, test_samples, epochs=EPOCHS):\n",
    "    test_size = max(1, int(len(train_samples) * VALIDATION_SPLIT))\n",
    "    labels = [s[\"label\"] for s in train_samples]\n",
    "    n_classes = len(set(labels))\n",
    "    use_stratify = test_size >= n_classes\n",
    "\n",
    "    train_subset, val_subset = train_test_split(\n",
    "        train_samples,\n",
    "        test_size=test_size,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=labels if use_stratify else None,\n",
    "    )\n",
    "\n",
    "    train_ds = make_dataset(train_subset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    val_ds = make_dataset(val_subset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    test_ds = make_dataset(test_samples, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = create_vtnet_dgm_model()\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    for (images, dgms), labels_batch in test_ds:\n",
    "        preds = model.predict([images, dgms], verbose=0)\n",
    "        y_true.extend(np.argmax(labels_batch.numpy(), axis=1))\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred, labels=[0, 1]),\n",
    "    }\n",
    "\n",
    "    return metrics, history.history, y_true, y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164125bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_louo_for_graph(graph_type: str):\n",
    "    samples = collect_samples_for_graph(graph_type)\n",
    "    if len(samples) == 0:\n",
    "        raise ValueError(f\"No samples found for graph type {graph_type}\")\n",
    "\n",
    "    df = pd.DataFrame(samples)\n",
    "    user_ids = sorted(df[\"user_id\"].unique())\n",
    "\n",
    "    overall_true, overall_pred = [], []\n",
    "    fold_results = {}\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        train_samples = df[df[\"user_id\"] != user_id].to_dict(\"records\")\n",
    "        test_samples = df[df[\"user_id\"] == user_id].to_dict(\"records\")\n",
    "        if len(train_samples) == 0 or len(test_samples) == 0:\n",
    "            continue\n",
    "\n",
    "        metrics, history, y_true, y_pred = run_fold(train_samples, test_samples)\n",
    "        fold_results[user_id] = {\"metrics\": metrics, \"history\": history}\n",
    "\n",
    "        overall_true.extend(y_true)\n",
    "        overall_pred.extend(y_pred)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        overall_true, overall_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    aggregate = {\n",
    "        \"accuracy\": accuracy_score(overall_true, overall_pred),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": confusion_matrix(overall_true, overall_pred, labels=[0, 1]),\n",
    "        \"folds\": fold_results,\n",
    "    }\n",
    "    return aggregate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ef5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for graph in graph_types:\n",
    "    print(f\"\\n=== Running VTNet DGM LOUO for {graph} graphs ===\")\n",
    "    agg = run_louo_for_graph(graph)\n",
    "    results[graph] = agg\n",
    "    print(\n",
    "        f\"Accuracy: {agg['accuracy']:.3f} | Precision: {agg['precision']:.3f} | \"\n",
    "        f\"Recall: {agg['recall']:.3f} | F1: {agg['f1']:.3f}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"graph_type\": graph,\n",
    "            \"accuracy\": res[\"accuracy\"],\n",
    "            \"precision\": res[\"precision\"],\n",
    "            \"recall\": res[\"recall\"],\n",
    "            \"f1\": res[\"f1\"],\n",
    "        }\n",
    "        for graph, res in results.items()\n",
    "    ]\n",
    ").sort_values(\"graph_type\")\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"bar\"\n",
    "\n",
    "fold_rows = []\n",
    "for user_id, info in results.get(graph_type, {}).get(\"folds\", {}).items():\n",
    "    m = info[\"metrics\"]\n",
    "    fold_rows.append(\n",
    "        {\n",
    "            \"user_id\": user_id,\n",
    "            \"accuracy\": m[\"accuracy\"],\n",
    "            \"precision\": m[\"precision\"],\n",
    "            \"recall\": m[\"recall\"],\n",
    "            \"f1\": m[\"f1\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "pd.DataFrame(fold_rows).sort_values(\"user_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfafdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"bar\"\n",
    "\n",
    "if graph_type in results:\n",
    "    for user_id, info in results[graph_type][\"folds\"].items():\n",
    "        cm = info[\"metrics\"][\"confusion_matrix\"]\n",
    "        plt.figure(figsize=(3.5, 3))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "            yticklabels=[\"True 0\", \"True 1\"],\n",
    "        )\n",
    "        plt.title(f\"{graph_type} | User {user_id}\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Graph type '{graph_type}' not in results.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c48190",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_accuracy = {}\n",
    "\n",
    "for graph_type in graph_types:\n",
    "    if graph_type not in results:\n",
    "        continue\n",
    "\n",
    "    correct_users = 0\n",
    "    total_users = 0\n",
    "    user_details = []\n",
    "\n",
    "    for user_id, info in results[graph_type][\"folds\"].items():\n",
    "        user_acc = info[\"metrics\"][\"accuracy\"]\n",
    "        total_users += 1\n",
    "        is_correct = user_acc > 0.5\n",
    "        if is_correct:\n",
    "            correct_users += 1\n",
    "        user_details.append(\n",
    "            {\n",
    "                \"user_id\": user_id,\n",
    "                \"accuracy\": user_acc,\n",
    "                \"correct\": is_correct,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    new_accuracy = correct_users / total_users if total_users else 0.0\n",
    "    user_level_accuracy[graph_type] = {\n",
    "        \"new_accuracy\": new_accuracy,\n",
    "        \"correct_users\": correct_users,\n",
    "        \"total_users\": total_users,\n",
    "        \"user_details\": user_details,\n",
    "    }\n",
    "\n",
    "    print(f\"{graph_type} graphs:\")\n",
    "    print(f\"  New Accuracy: {new_accuracy:.3f} ({correct_users}/{total_users} users)\\n\")\n",
    "\n",
    "summary_user_level = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"graph_type\": graph,\n",
    "            \"new_accuracy\": info[\"new_accuracy\"],\n",
    "            \"correct_users\": info[\"correct_users\"],\n",
    "            \"total_users\": info[\"total_users\"],\n",
    "        }\n",
    "        for graph, info in user_level_accuracy.items()\n",
    "    ]\n",
    ").sort_values(\"graph_type\")\n",
    "\n",
    "print(\"Summary:\")\n",
    "summary_user_level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf2f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
