{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51856c4",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bcdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Fix for torch.utils._pytree compatibility issue\n",
    "if not hasattr(torch.utils._pytree, 'register_pytree_node'):\n",
    "    torch.utils._pytree.register_pytree_node = lambda *args, **kwargs: None\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798acc78",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120325f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "DATA_PATH = \"Organized Normalized Tumbling Window DGMs (3s)/all\"\n",
    "FEATURE_COLUMNS = None\n",
    "\n",
    "# BERT configuration\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 50\n",
    "MAX_GRAD_NORM = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30a7fe",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing with User ID Extraction\n",
    "\n",
    "The DGM data is organized as:\n",
    "```\n",
    "all/\n",
    "├── literate/\n",
    "│   ├── user_1/\n",
    "│   │   ├── user_1_question_10_ID_29_tumbling_all_window_DGMs.csv\n",
    "│   │   ├── user_1_question_12_ID_15_tumbling_all_window_DGMs.csv\n",
    "│   │   └── ...\n",
    "│   ├── user_2/\n",
    "│   └── ...\n",
    "└── illiterate/\n",
    "    ├── user_1/\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512, normalize=True):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Load all data files\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.features = []\n",
    "        self.user_ids = []\n",
    "        \n",
    "        self._load_data()\n",
    "        self._preprocess_features()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load CSV files from organized folder structure and extract user IDs\n",
    "        Treat each CSV file as one sample by aggregating its rows (column-wise mean).\n",
    "        \"\"\"\n",
    "        print(\"Loading DGM data files...\")\n",
    "        \n",
    "        # Define label mapping\n",
    "        label_map = {'literate': 0, 'illiterate': 1}\n",
    "        \n",
    "        for label_name, label_value in label_map.items():\n",
    "            label_path = os.path.join(self.data_path, label_name)\n",
    "            \n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"Warning: {label_path} does not exist\")\n",
    "                continue\n",
    "            \n",
    "            # Get all user folders\n",
    "            user_folders = [d for d in os.listdir(label_path) \n",
    "                          if os.path.isdir(os.path.join(label_path, d)) and d.startswith('user_')]\n",
    "            \n",
    "            for user_folder in user_folders:\n",
    "                # Extract user ID from folder name (e.g., 'user_1' -> 1)\n",
    "                match = re.search(r'user_(\\d+)', user_folder)\n",
    "                if not match:\n",
    "                    continue\n",
    "                user_id = int(match.group(1))\n",
    "                \n",
    "                user_path = os.path.join(label_path, user_folder)\n",
    "                csv_files = glob.glob(os.path.join(user_path, '*.csv'))\n",
    "                \n",
    "                for csv_file in csv_files:\n",
    "                    try:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "\n",
    "                        # Treat each file as one sample by aggregating its rows.\n",
    "                        # Convert all columns to numeric where possible, replace non-numeric with 0,\n",
    "                        # then compute the column-wise mean to produce a single feature vector per file.\n",
    "                        numeric_df = df.apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "\n",
    "                        # If there are no numeric columns, use an empty feature vector\n",
    "                        if numeric_df.shape[1] == 0:\n",
    "                            agg_features = np.array([])\n",
    "                        else:\n",
    "                            agg_features = numeric_df.mean(axis=0).values\n",
    "\n",
    "                        # Append one sample per file\n",
    "                        self.samples.append(csv_file)\n",
    "                        self.labels.append(label_value)\n",
    "                        self.features.append(agg_features)\n",
    "                        self.user_ids.append(user_id)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {csv_file}: {e}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} samples from {len(set(self.user_ids))} users\")\n",
    "        print(f\"Label distribution: {pd.Series(self.labels).value_counts().to_dict()}\")\n",
    "        \n",
    "    def _preprocess_features(self):\n",
    "        \"\"\"Normalize features and convert to text\"\"\"\n",
    "        print(\"Preprocessing features...\")\n",
    "\n",
    "        # Convert to numpy array for easier manipulation\n",
    "        # Handle variable-length feature vectors by padding with zeros to the max length\n",
    "        if len(self.features) == 0:\n",
    "            features_array = np.zeros((0, 0), dtype=float)\n",
    "        else:\n",
    "            max_len = max([len(f) for f in self.features])\n",
    "            features_array = np.zeros((len(self.features), max_len), dtype=float)\n",
    "            for i, f in enumerate(self.features):\n",
    "                if len(f) > 0:\n",
    "                    features_array[i, :len(f)] = f\n",
    "\n",
    "        if self.normalize:\n",
    "            # Normalize each feature to [0, 1] range\n",
    "            # Handle NaN and inf values\n",
    "            features_array = np.nan_to_num(features_array, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "            \n",
    "            # Min-max normalization per feature\n",
    "            if features_array.size == 0:\n",
    "                min_vals = np.array([])\n",
    "                max_vals = np.array([])\n",
    "            else:\n",
    "                min_vals = np.min(features_array, axis=0)\n",
    "                max_vals = np.max(features_array, axis=0)\n",
    "                \n",
    "            # Avoid division by zero\n",
    "            range_vals = max_vals - min_vals if features_array.size != 0 else np.array([])\n",
    "            if range_vals.size != 0:\n",
    "                range_vals[range_vals == 0] = 1.0\n",
    "                features_array = (features_array - min_vals) / range_vals\n",
    "\n",
    "        # Convert features to text representation (rounded to 3 decimals)\n",
    "        self.text_features = []\n",
    "        for features in features_array:\n",
    "            # Convert numerical features to space-separated string\n",
    "            text = ' '.join([f\"{val:.3f}\" for val in features])\n",
    "            self.text_features.append(text)\n",
    "        \n",
    "        print(\"Feature preprocessing complete\")\n",
    "        \n",
    "    def get_user_ids(self):\n",
    "        \"\"\"Return list of all user IDs\"\"\"\n",
    "        return self.user_ids\n",
    "    \n",
    "    def get_unique_users(self):\n",
    "        \"\"\"Return sorted list of unique user IDs\"\"\"\n",
    "        return sorted(set(self.user_ids))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        user_id = self.user_ids[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'user_id': torch.tensor(user_id, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9461caa",
   "metadata": {},
   "source": [
    "## Initialize Dataset and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd33784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "dataset = DGMDataset(\n",
    "    data_path=DATA_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Number of unique users: {len(dataset.get_unique_users())}\")\n",
    "print(f\"User IDs: {dataset.get_unique_users()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284f503",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82045883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device, gradient_accumulation_steps=1):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item() * gradient_accumulation_steps})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a84162",
   "metadata": {},
   "source": [
    "## Leave-One-User-Out Cross-Validation\n",
    "\n",
    "For each user:\n",
    "1. Use that user's data as test set\n",
    "2. Use all other users' data as training set\n",
    "3. Train model from scratch\n",
    "4. Evaluate on held-out user\n",
    "5. Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique users\n",
    "unique_users = dataset.get_unique_users()\n",
    "all_user_ids = dataset.get_user_ids()\n",
    "\n",
    "print(f\"Starting Leave-One-User-Out Cross-Validation\")\n",
    "print(f\"Number of folds: {len(unique_users)}\")\n",
    "print(f\"Unique users: {unique_users}\")\n",
    "\n",
    "# Store results\n",
    "louo_results = []\n",
    "all_predictions = []\n",
    "\n",
    "# Perform LOUO CV\n",
    "for fold_idx, test_user in enumerate(unique_users):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Fold {fold_idx + 1}/{len(unique_users)} - Testing on User {test_user}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_indices = [i for i, uid in enumerate(all_user_ids) if uid != test_user]\n",
    "    test_indices = [i for i, uid in enumerate(all_user_ids) if uid == test_user]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_indices)}\")\n",
    "    print(f\"Test samples: {len(test_indices)}\")\n",
    "    \n",
    "    # Create subsets\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = len(train_loader) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=WARMUP_STEPS,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\nTraining for {NUM_EPOCHS} epochs...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n",
    "        )\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # Clear cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating on User {test_user}...\")\n",
    "    test_results = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\nUser {test_user} Results:\")\n",
    "    print(f\"  Accuracy:  {test_results['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {test_results['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {test_results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {test_results['f1']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    louo_results.append({\n",
    "        'user_id': test_user,\n",
    "        'accuracy': test_results['accuracy'],\n",
    "        'precision': test_results['precision'],\n",
    "        'recall': test_results['recall'],\n",
    "        'f1': test_results['f1'],\n",
    "        'n_samples': len(test_indices)\n",
    "    })\n",
    "    \n",
    "    # Store predictions\n",
    "    for i, (pred, true) in enumerate(zip(test_results['predictions'], test_results['true_labels'])):\n",
    "        all_predictions.append({\n",
    "            'user_id': test_user,\n",
    "            'sample_idx': test_indices[i],\n",
    "            'true_label': true,\n",
    "            'predicted_label': pred\n",
    "        })\n",
    "    \n",
    "    # Clean up\n",
    "    del model, optimizer, scheduler\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Leave-One-User-Out Cross-Validation Complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a25a7",
   "metadata": {},
   "source": [
    "## Aggregate Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad455aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(louo_results)\n",
    "predictions_df = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Calculate aggregate statistics\n",
    "mean_accuracy = results_df['accuracy'].mean()\n",
    "std_accuracy = results_df['accuracy'].std()\n",
    "mean_precision = results_df['precision'].mean()\n",
    "std_precision = results_df['precision'].std()\n",
    "mean_recall = results_df['recall'].mean()\n",
    "std_recall = results_df['recall'].std()\n",
    "mean_f1 = results_df['f1'].mean()\n",
    "std_f1 = results_df['f1'].std()\n",
    "\n",
    "print(\"\\nAggregate Results (Mean ± Std):\")\n",
    "print(f\"Accuracy:  {mean_accuracy:.4f} ± {std_accuracy:.4f}\")\n",
    "print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall:    {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"F1-Score:  {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "\n",
    "# Display per-user results\n",
    "print(\"\\nPer-User Results:\")\n",
    "print(results_df.sort_values('accuracy', ascending=False).to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('bert_dgms_3s_louo_results.csv', index=False)\n",
    "predictions_df.to_csv('bert_dgms_3s_louo_predictions.csv', index=False)\n",
    "print(\"\\nResults saved to:\")\n",
    "print(\"  - bert_dgms_3s_louo_results.csv\")\n",
    "print(\"  - bert_dgms_3s_louo_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208fca3",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('BERT DGMs (3s) - Leave-One-User-Out Cross-Validation Results', fontsize=16)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].hist(results_df['accuracy'], bins=20, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(mean_accuracy, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_accuracy:.3f}')\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Accuracy Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Precision\n",
    "axes[0, 1].hist(results_df['precision'], bins=20, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].axvline(mean_precision, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_precision:.3f}')\n",
    "axes[0, 1].set_xlabel('Precision')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Precision Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Recall\n",
    "axes[1, 0].hist(results_df['recall'], bins=20, color='lightcoral', edgecolor='black')\n",
    "axes[1, 0].axvline(mean_recall, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_recall:.3f}')\n",
    "axes[1, 0].set_xlabel('Recall')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Recall Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# F1-Score\n",
    "axes[1, 1].hist(results_df['f1'], bins=20, color='plum', edgecolor='black')\n",
    "axes[1, 1].axvline(mean_f1, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_f1:.3f}')\n",
    "axes[1, 1].set_xlabel('F1-Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('F1-Score Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bert_dgms_3s_louo_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved to: bert_dgms_3s_louo_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad4432",
   "metadata": {},
   "source": [
    "## Box Plot of Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "metrics_data = results_df[['accuracy', 'precision', 'recall', 'f1']]\n",
    "\n",
    "bp = plt.boxplot([metrics_data['accuracy'], metrics_data['precision'], \n",
    "                   metrics_data['recall'], metrics_data['f1']],\n",
    "                  labels=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "                  patch_artist=True)\n",
    "\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'plum']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.title('BERT DGMs (3s) - Performance Metrics Distribution (LOUO)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.savefig('bert_dgms_3s_louo_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Box plot saved to: bert_dgms_3s_louo_boxplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158b38f",
   "metadata": {},
   "source": [
    "## Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDetailed Statistical Summary:\")\n",
    "print(\"=\"*60)\n",
    "summary_stats = results_df[['accuracy', 'precision', 'recall', 'f1']].describe()\n",
    "print(summary_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Top 5 Users by Accuracy:\")\n",
    "print(results_df.nlargest(5, 'accuracy')[['user_id', 'accuracy', 'f1', 'n_samples']])\n",
    "\n",
    "print(\"\\nBottom 5 Users by Accuracy:\")\n",
    "print(results_df.nsmallest(5, 'accuracy')[['user_id', 'accuracy', 'f1', 'n_samples']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
